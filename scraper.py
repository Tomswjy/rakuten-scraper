import pandas as pd
from curl_cffi import requests
from bs4 import BeautifulSoup
import time
import random
import re
import os
import io
from deep_translator import GoogleTranslator
from urllib.parse import unquote
from openpyxl.drawing.image import Image as XLImage
from PIL import Image as PILImage
from openai import OpenAI

# ==================== âš™ï¸ é…ç½®åŒºåŸŸ ====================
KEYWORD = "ã‚³ã‚«ã‚³ãƒ¼ãƒ©"
PAGES_TO_SCRAPE = 1
# âš ï¸âš ï¸âš ï¸ è¯·åŠ¡å¿…ç¡®è®¤æ‚¨çš„ä»£ç†ç«¯å£
PROXY_PORT = 7897
# æ·±åº¦æŠ“å–é™åˆ¶ (None=æŠ“å–æœ¬é¡µå…¨éƒ¨, 5=åªæµ‹å‰5ä¸ª)
DEEP_SCRAPE_LIMIT = 5

# ==================== ğŸ¤– AIé…ç½® ====================
# API Key ä»ç¯å¢ƒå˜é‡è·å–ï¼Œæˆ–ç›´æ¥å¡«å†™
AI_API_KEY = "sk-w8xPygiSwsGuoN5yeddcH373PofeRw5Vxcb3yhPmD92ga2UL"
AI_BASE_URL = "https://api.probex.top/v1"
AI_MODEL = "Qwen3-VL-235B-A22B-Instruct"
# æ˜¯å¦å¯ç”¨AIæå–ç‰¹å¾ (å¦‚æœAPI Keyä¸ºç©ºåˆ™è‡ªåŠ¨ç¦ç”¨)
USE_AI_FEATURES = bool(AI_API_KEY)

# åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆèµ°ä»£ç†è®¿é—®APIï¼‰
import httpx

ai_client = None
if AI_API_KEY:
    try:
        # åˆ›å»ºèµ°ä»£ç†çš„ http å®¢æˆ·ç«¯
        http_client = httpx.Client(
            timeout=60.0,
            proxy=f"http://127.0.0.1:{PROXY_PORT}"
        )
        ai_client = OpenAI(
            api_key=AI_API_KEY,
            base_url=AI_BASE_URL,
            http_client=http_client
        )
        print(f"âœ… AIç‰¹å¾æå–å·²å¯ç”¨ (æ¨¡å‹: {AI_MODEL})")
    except Exception as e:
        print(f"âš ï¸ AIåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å…³é”®è¯åŒ¹é…")
        USE_AI_FEATURES = False
else:
    print("âš ï¸ æœªè®¾ç½® AI_API_KEYï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…æå–ç‰¹å¾")
# åˆ—è¡¨é¡µæŠ“å–é™åˆ¶ (None=æŠ“å–æœ¬é¡µå…¨éƒ¨, 5=åªå–å‰5ä¸ª)
LIST_SCRAPE_LIMIT = 5
# æ˜¯å¦å¯ç”¨ç¿»è¯‘ (True=ç¿»è¯‘, False=ä¸ç¿»è¯‘ï¼Œé¿å…å´©æºƒ)
ENABLE_TRANSLATION = True
# æ˜¯å¦ä¸‹è½½å›¾ç‰‡å¹¶åµŒå…¥Excel (True=ä¸‹è½½åµŒå…¥, False=åªä¿å­˜é“¾æ¥)
DOWNLOAD_IMAGES = True
# å›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹
IMAGE_FOLDER = "product_images"
# ====================================================

proxies = {
    "http": f"http://127.0.0.1:{PROXY_PORT}",
    "https": f"http://127.0.0.1:{PROXY_PORT}"
}


def safe_translate(text):
    """å®‰å…¨ç¿»è¯‘å‡½æ•°ï¼Œå¸¦é‡è¯•å’Œå»¶è¿Ÿ"""
    if not text or str(text) == "nan" or text == "N/A" or text == "" or text == "...":
        return text
    try:
        text = str(text).strip()
        if len(text) < 2:  # å¤ªçŸ­ä¸ç¿»è¯‘
            return text
        # é™åˆ¶é•¿åº¦é˜²æ­¢ç¿»è¯‘APIæŠ¥é”™
        text = text[:800]
        time.sleep(0.3)  # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        result = GoogleTranslator(source='ja', target='zh-CN').translate(text)
        return result if result else text
    except Exception as e:
        # ç¿»è¯‘å¤±è´¥è¿”å›åŸæ–‡
        return text


def extract_features_from_images(image_urls, max_images=3):
    """
    ã€è§†è§‰AIç‰¹å¾æå–ã€‘ä½¿ç”¨Qwen VLä»å•†å“å›¾ç‰‡ä¸­æå–äº§å“ç‰¹å¾
    """
    global ai_client
    if not ai_client or not USE_AI_FEATURES or not image_urls:
        return []

    features = []
    import base64
    from io import BytesIO

    for img_url in image_urls[:max_images]:
        try:
            # ä¸‹è½½å›¾ç‰‡
            resp = requests.get(img_url, impersonate="chrome120", proxies=proxies, timeout=10)
            if resp.status_code != 200:
                continue

            # è½¬ä¸ºbase64
            img_b64 = base64.b64encode(resp.content).decode('utf-8')

            # è°ƒç”¨è§†è§‰AI
            response = ai_client.chat.completions.create(
                model=AI_MODEL,
                temperature=0,
                max_tokens=200,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯äº§å“ç‰¹å¾è¯†åˆ«åŠ©æ‰‹ã€‚è¯·è¯†åˆ«å›¾ç‰‡ä¸­å±•ç¤ºçš„äº§å“ç‰¹å¾ï¼Œç”¨ç®€çŸ­ä¸­æ–‡æè¿°ï¼ˆ2-6å­—ï¼‰ï¼Œé€—å·åˆ†éš”ã€‚"},
                    {"role": "user", "content": [
                        {"type": "text", "text": "è¯·ä»è¿™å¼ å•†å“å›¾ç‰‡ä¸­æå–äº§å“ç‰¹å¾ï¼Œå¦‚æè´¨ã€é¢œè‰²ã€ç»“æ„ã€åŠŸèƒ½ç­‰ã€‚åªè¾“å‡ºç‰¹å¾è¯ï¼Œç”¨é€—å·åˆ†éš”ã€‚"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
                    ]}
                ]
            )
            result = response.choices[0].message.content.strip()
            # è§£æ
            img_features = [f.strip() for f in result.split(',') if f.strip() and len(f.strip()) <= 15]
            for f in img_features:
                if f not in features:
                    features.append(f)

            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        except Exception as e:
            print(f"      [AIå›¾ç‰‡] åˆ†æå¤±è´¥: {e}")
            continue

    return features[:9]


def extract_product_features_ai(title, description, specs, image_urls=None):
    """
    ã€çœŸAIç‰¹å¾æå–ã€‘ä½¿ç”¨Qwenå¤§æ¨¡å‹ä»å•†å“ä¿¡æ¯ä¸­æå–9ä¸ªæ ¸å¿ƒäº§å“ç‰¹å¾
    æ”¯æŒæ–‡æœ¬+å›¾ç‰‡è”åˆåˆ†æ
    """
    global ai_client
    if not ai_client or not USE_AI_FEATURES:
        return None

    features = []

    # 1. ä»å›¾ç‰‡æå–ç‰¹å¾ï¼ˆå¦‚æœæœ‰å›¾ç‰‡URLï¼‰
    if image_urls:
        img_features = extract_features_from_images(image_urls, max_images=2)
        features.extend(img_features)
        if img_features:
            print(f"      [AIå›¾ç‰‡] æå–åˆ°ç‰¹å¾: {img_features[:5]}")

    # 2. ä»æ–‡æœ¬æå–ç‰¹å¾
    prompt = f"""è¯·ä»ä»¥ä¸‹æ—¥æœ¬ä¹å¤©å•†å“ä¿¡æ¯ä¸­æå–æœ€å¤š9ä¸ªæ ¸å¿ƒäº§å“å–ç‚¹ç‰¹å¾ã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªç‰¹å¾ç”¨ç®€çŸ­ä¸­æ–‡æè¿°ï¼ˆ2-8ä¸ªå­—ï¼‰
2. æå–äº§å“çš„æ ¸å¿ƒå–ç‚¹ï¼Œå¦‚ï¼šæè´¨ã€åŠŸèƒ½ã€è®¾è®¡ã€é€‚ç”¨åœºæ™¯ç­‰
3. å‚è€ƒç¤ºä¾‹ï¼šå¼€æ”¾å¼æ”¶çº³ã€é˜²é”ˆæ¶‚å±‚ã€åšå›ºè€ç”¨ã€å¸¦è½®å¯ç§»åŠ¨ã€é«˜åº¦å¯è°ƒã€ç»„è£…ç®€å•ã€å¤§å®¹é‡ã€å¤šå±‚è®¾è®¡ã€èŠ‚çœç©ºé—´

å•†å“æ ‡é¢˜ï¼š{title[:200]}
å•†å“æè¿°ï¼š{description[:500]}
å•†å“å‚æ•°ï¼š{specs[:400]}

è¯·ç›´æ¥è¿”å›9ä¸ªç‰¹å¾ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

    try:
        response = ai_client.chat.completions.create(
            model=AI_MODEL,
            temperature=0,
            max_tokens=200,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªäº§å“ç‰¹å¾æå–åŠ©æ‰‹ï¼Œåªè¾“å‡ºç®€çŸ­çš„ä¸­æ–‡ç‰¹å¾è¯ï¼Œç”¨é€—å·åˆ†éš”ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        result = response.choices[0].message.content.strip()
        # è§£æç»“æœ
        text_features = [f.strip() for f in result.split(',') if f.strip() and len(f.strip()) <= 15]
        for f in text_features:
            if f not in features:
                features.append(f)
    except Exception as e:
        print(f"      [AIæ–‡æœ¬] è°ƒç”¨å¤±è´¥: {e}")

    return features[:9] if features else None


def extract_product_features_keywords(title, description, specs):
    """
    ã€å…³é”®è¯åŒ¹é…ã€‘å¤‡ç”¨æ–¹æ¡ˆï¼šä»å•†å“ä¿¡æ¯ä¸­ç”¨å…³é”®è¯åŒ¹é…æå–ç‰¹å¾
    """
    features = []

    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
    all_text = f"{title} {description} {specs}".lower()

    # ç‰¹å¾å…³é”®è¯åº“ (æ—¥æ–‡å…³é”®è¯ -> ä¸­æ–‡ç‰¹å¾)
    feature_keywords = {
        # æ”¶çº³æ–¹å¼
        "ã‚ªãƒ¼ãƒ—ãƒ³|é–‹æ”¾|è¦‹ã›ã‚‹åç´": "å¼€æ”¾å¼æ”¶çº³",
        "æ‰‰ä»˜ã|å¼•ãå‡ºã—|éš ã™åç´": "å°é—­å¼æ”¶çº³",
        # æè´¨
        "ã‚¹ãƒãƒ¼ãƒ«|é‰„|é‡‘å±|ãƒ¡ã‚¿ãƒ«": "é’¢é“æè´¨",
        "æœ¨è£½|ã‚¦ãƒƒãƒ‰|å¤©ç„¶æœ¨": "æœ¨è´¨æè´¨",
        "ãƒ—ãƒ©ã‚¹ãƒãƒƒã‚¯|æ¨¹è„‚": "å¡‘æ–™æè´¨",
        "ã‚¹ãƒ†ãƒ³ãƒ¬ã‚¹": "ä¸é”ˆé’¢æè´¨",
        # è¡¨é¢å¤„ç†
        "é˜²éŒ†|ã‚µãƒ“é˜²æ­¢|éŒ†ã³ã«ãã„|ç²‰ä½“å¡—è£…": "é˜²é”ˆæ¶‚å±‚",
        "é˜²æ°´|è€æ°´": "é˜²æ°´å¤„ç†",
        "ã‚¯ãƒ­ãƒ ãƒ¡ãƒƒã‚­|ãƒ¡ãƒƒã‚­": "ç”µé•€å¤„ç†",
        # ç»“æ„å¼ºåº¦
        "é ‘ä¸ˆ|ä¸ˆå¤«|è€è·é‡|å¼·ã„|å …ç‰¢": "åšå›ºè€ç”¨",
        "è»½é‡|è»½ã„": "è½»ä¾¿",
        # ç§»åŠ¨æ€§
        "ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼|è»Šè¼ª|ç§»å‹•": "å¸¦è½®å¯ç§»åŠ¨",
        "ã‚¢ã‚¸ãƒ£ã‚¹ã‚¿ãƒ¼|å›ºå®šè„š|å®‰å®š": "å›ºå®šè„šç¨³å®š",
        # å¯è°ƒèŠ‚
        "é«˜ã•èª¿ç¯€|èª¿æ•´å¯èƒ½|å¯å‹•": "é«˜åº¦å¯è°ƒ",
        "æ£šæ¿.*èª¿ç¯€|æ®µéšèª¿æ•´": "å±‚æ¿å¯è°ƒ",
        # é€æ°”æ€§
        "ãƒ¡ãƒƒã‚·ãƒ¥|ç¶²|é€šæ°—|é€šé¢¨": "ç½‘çŠ¶é€æ°”",
        "ãƒ¯ã‚¤ãƒ¤ãƒ¼": "é’¢ä¸ç½‘ç»“æ„",
        # ç»„è£…
        "ç°¡å˜çµ„ç«‹|å·¥å…·ä¸è¦|ãƒ¯ãƒ³ã‚¿ãƒƒãƒ": "ç»„è£…ç®€å•",
        "çµ„ã¿ç«‹ã¦å¼": "éœ€ç®€å•ç»„è£…",
        # å¤–è§‚
        "ãŠã—ã‚ƒã‚Œ|ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥|ãƒ¢ãƒ€ãƒ³": "å¤–è§‚æ—¶å°š",
        "çœã‚¹ãƒšãƒ¼ã‚¹|ã‚¹ãƒªãƒ |ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆ": "èŠ‚çœç©ºé—´",
        # ç”¨é€”
        "ã‚­ãƒƒãƒãƒ³|å°æ‰€": "é€‚åˆå¨æˆ¿",
        "æ´—é¢æ‰€|æµ´å®¤|ãƒã‚¹": "é€‚åˆæµ´å®¤",
        "æŠ¼å…¥ã‚Œ|ã‚¯ãƒ­ãƒ¼ã‚¼ãƒƒãƒˆ": "é€‚åˆè¡£æŸœ",
        "ãƒªãƒ“ãƒ³ã‚°|å±…é–“": "é€‚åˆå®¢å…",
        # å…¶ä»–ç‰¹ç‚¹
        "å¤§å®¹é‡|ãŸã£ã·ã‚Šåç´": "å¤§å®¹é‡",
        "å¤šæ©Ÿèƒ½|å¤šç”¨é€”": "å¤šåŠŸèƒ½",
        "ä¼¸ç¸®|æ‹¡å¼µ": "å¯ä¼¸ç¼©",
        # æ–°å¢ï¼šå±‚æ•°/æ®µæ•°
        r"\d+æ®µ|\d+å±¤": "å¤šå±‚è®¾è®¡",
        "4æ®µ|å››æ®µ": "4å±‚è®¾è®¡",
        "5æ®µ|äº”æ®µ": "5å±‚è®¾è®¡",
        # æ–°å¢ï¼šå“ç‰Œ/å“è´¨
        "ãƒ«ãƒŸãƒŠã‚¹|luminous": "çŸ¥åå“ç‰Œ",
        "æ—¥æœ¬è£½|å›½ç”£": "æ—¥æœ¬åˆ¶é€ ",
        "æ¥­å‹™ç”¨|ãƒ—ãƒ­": "ä¸“ä¸šçº§",
        # æ–°å¢ï¼šå°ºå¯¸
        r"å¹…\d+|æ¨ªå¹…": "å®½åº¦é€‚ä¸­",
        r"å¥¥è¡Œ\d+": "æ·±åº¦é€‚ä¸­",
        r"é«˜ã•\d+cm": "é«˜åº¦é€‚ä¸­",
    }

    # åŒ¹é…ç‰¹å¾
    for keywords, feature in feature_keywords.items():
        if re.search(keywords, all_text, re.I):
            if feature not in features:
                features.append(feature)

    # å¦‚æœç‰¹å¾ä¸è¶³ï¼Œå°è¯•ä»å‚æ•°ä¸­æå–æ›´å¤šä¿¡æ¯
    if len(features) < 5:
        # å°ºå¯¸ä¿¡æ¯
        size_match = re.search(r'(å¹…|å¥¥è¡Œ|é«˜ã•)[^\d]*(\d+)', all_text)
        if size_match and "å°ºå¯¸è§„æ ¼" not in features:
            features.append("å°ºå¯¸è§„æ ¼æ˜ç¡®")

        # é¢œè‰²
        colors = re.findall(r'(ãƒ–ãƒ©ãƒƒã‚¯|ãƒ›ãƒ¯ã‚¤ãƒˆ|ã‚·ãƒ«ãƒãƒ¼|ãƒ–ãƒ©ã‚¦ãƒ³|ãƒŠãƒãƒ¥ãƒ©ãƒ«|é»’|ç™½|éŠ€)', all_text)
        if colors and "å¤šè‰²å¯é€‰" not in features:
            features.append("å¤šè‰²å¯é€‰" if len(set(colors)) > 1 else f"é¢œè‰²ç®€çº¦")

    return features[:9]


def extract_product_features(title, description, specs, image_urls=None):
    """
    ã€æ™ºèƒ½ç‰¹å¾æå–ã€‘ä¼˜å…ˆä½¿ç”¨AIï¼ˆæ”¯æŒå›¾ç‰‡åˆ†æï¼‰ï¼Œå¤±è´¥åˆ™ç”¨å…³é”®è¯åŒ¹é…
    """
    # ä¼˜å…ˆä½¿ç”¨AIï¼ˆåŒ…å«å›¾ç‰‡åˆ†æï¼‰
    if USE_AI_FEATURES:
        ai_features = extract_product_features_ai(title, description, specs, image_urls)
        if ai_features:
            return ai_features

    # å¤‡ç”¨ï¼šå…³é”®è¯åŒ¹é…
    return extract_product_features_keywords(title, description, specs)


def download_image(img_url, save_path):
    """
    ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°
    è¿”å›: æœ¬åœ°æ–‡ä»¶è·¯å¾„ æˆ– None
    """
    if not img_url or img_url == "N/A":
        return None
    try:
        resp = requests.get(img_url, impersonate="chrome120", proxies=proxies, timeout=10)
        if resp.status_code == 200:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)

            # ç”¨PILå¤„ç†å›¾ç‰‡ï¼ˆç»Ÿä¸€è½¬ä¸ºPNGï¼Œè°ƒæ•´å¤§å°ï¼‰
            img_data = io.BytesIO(resp.content)
            img = PILImage.open(img_data)

            # è½¬æ¢ä¸ºRGBï¼ˆå¤„ç†RGBAæˆ–å…¶ä»–æ¨¡å¼ï¼‰
            if img.mode in ('RGBA', 'P'):
                img = img.convert('RGB')

            # è°ƒæ•´å¤§å°ï¼ˆExcelæ˜¾ç¤ºç”¨ï¼Œå®½åº¦80pxï¼‰
            max_width = 80
            ratio = max_width / img.width
            new_height = int(img.height * ratio)
            img = img.resize((max_width, new_height), PILImage.Resampling.LANCZOS)

            # ä¿å­˜ä¸ºJPEG
            save_path = save_path.rsplit('.', 1)[0] + '.jpg'
            img.save(save_path, 'JPEG', quality=85)
            return save_path
    except Exception as e:
        pass
    return None


def resolve_pr_link(url: str) -> str:
    """
    ã€ä¿®å¤ã€‘å¼ºåŠ›è§£æ PR å¹¿å‘Šé“¾æ¥ (æ”¯æŒ Meta Refresh å’Œ JS è·³è½¬)
    """
    if not url: return ""
    if "item.rakuten.co.jp" in url and "redirect" not in url: return url

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        resp = requests.get(
            url,
            impersonate="chrome120",
            proxies=proxies,
            timeout=15,
            allow_redirects=True,
            headers=headers
        )

        if "item.rakuten.co.jp" in resp.url: return resp.url

        soup = BeautifulSoup(resp.text, 'html.parser')

        # æ–¹æ³•1: Meta Refresh
        meta = soup.find("meta", attrs={"http-equiv": re.compile("refresh", re.I)})
        if meta:
            content = meta.get("content", "")
            m = re.search(r'url=([^"]+)', content, re.I)
            if m: return resolve_pr_link(m.group(1))

        # æ–¹æ³•2: JS window.location.replace
        js = re.search(r'window\.location\.replace\("([^"]+)"\)', resp.text)
        if js: return resolve_pr_link(js.group(1))

        # æ–¹æ³•3: JS window.location.href æˆ– location.href
        js2 = re.search(r'(?:window\.)?location\.href\s*=\s*["\']([^"\']+)["\']', resp.text)
        if js2: return resolve_pr_link(js2.group(1))

        # æ–¹æ³•4: åœ¨é¡µé¢å†…æ‰¾ item.rakuten.co.jp é“¾æ¥
        lk = re.search(r'href="([^"]*item\.rakuten\.co\.jp[^"]*)"', resp.text)
        if lk: return lk.group(1)

        # æ–¹æ³•5: ä» JSON æ•°æ®ä¸­æå–ï¼ˆä¹å¤©å¹¿å‘Šç»å¸¸ç”¨JSONï¼‰
        json_url = re.search(r'"(?:url|link|href)"\s*:\s*"(https?://item\.rakuten\.co\.jp[^"]+)"', resp.text)
        if json_url: return json_url.group(1)

        # æ–¹æ³•6: ä»ä»»æ„ä½ç½®æå– item.rakuten.co.jp é“¾æ¥
        any_item = re.search(r'(https?://item\.rakuten\.co\.jp/[^/]+/[^/\s"\'<>]+)', resp.text)
        if any_item: return any_item.group(1)

        return url
    except Exception as e:
        print(f"      [DEBUG] resolve_pr_link å¼‚å¸¸: {e}")
        return url


def extract_dynamic_specs(soup):
    """
    ã€æ ¸å¿ƒåŠŸèƒ½ã€‘åŠ¨æ€æŠ“å–è¯¦æƒ…é¡µçš„æ‰€æœ‰è¡¨æ ¼å‚æ•°
    """
    specs_list = []
    tables = soup.find_all('table')
    for table in tables:
        rows = table.find_all('tr')
        for row in rows:
            cols = row.find_all(['th', 'td'])
            if len(cols) == 2:
                key = cols[0].get_text(strip=True)
                val = cols[1].get_text(strip=True)
                if len(key) < 30 and len(val) < 200 and val:
                    if any(x in key for x in ["é…é€", "æ”¯æ‰•", "é€æ–™", "ã‚«ãƒ¼ãƒ‰", "ã‚ã™æ¥½"]): continue
                    specs_list.append(f"ã€{key}ã€‘: {val}")

    if not specs_list:
        lis = soup.select('.item_desc li')
        for li in lis:
            t = li.get_text(strip=True)
            if ":" in t or "ï¼š" in t: specs_list.append(t)

    if specs_list:
        return "\n".join(specs_list[:20])
    return "æœªæŠ“å–åˆ°å‚æ•°"


# ==================== âœ¨ æ–°å¢åŠŸèƒ½å‡½æ•°åŒºåŸŸ âœ¨ ====================

def check_has_video(soup, raw_html):
    """æ£€æµ‹é¡µé¢æ˜¯å¦åŒ…å«è§†é¢‘"""
    # 1. æ£€æŸ¥ video æ ‡ç­¾
    if soup.find("video"): return "æœ‰"
    # 2. æ£€æŸ¥ä¹å¤©ä¸“ç”¨æ’­æ”¾å™¨ class
    if soup.select(".rakutenVideoPlayer"): return "æœ‰"
    # 3. æ£€æŸ¥ HTML æºç ä¸­çš„è§†é¢‘ç‰¹å¾
    if "rakuten.co.jp/rms/mall/image/video" in raw_html: return "æœ‰"
    # 4. æ£€æŸ¥ iframe (Youtube/Vimeo)
    iframes = soup.find_all("iframe")
    for iframe in iframes:
        src = iframe.get("src", "")
        if "youtube" in src or "vimeo" in src: return "æœ‰"
    return "æ— "


def get_categories_from_page(soup, raw_html=""):
    """
    ä»å•†å“è¯¦æƒ…é¡µè‡ªåŠ¨æå–ç±»ç›®IDå’Œåç§°
    è¿”å›: [{"id": "566374", "name": "ã‚¹ãƒãƒ¼ãƒ«ãƒ©ãƒƒã‚¯"}, ...]
    """
    categories = []
    page_text = raw_html if raw_html else str(soup)

    def add_category(cat_id, name):
        """æ·»åŠ ç±»ç›®ï¼ˆå»é‡ï¼‰"""
        if cat_id and len(cat_id) >= 5 and not any(c['id'] == cat_id for c in categories):
            categories.append({"id": cat_id, "name": name or f"ç±»ç›®{cat_id}"})

    try:
        # æ–¹æ³•1: ä»é¢åŒ…å±‘å¯¼èˆªæå– (/category/)
        for bc in soup.select('a[href*="/category/"]'):
            href = bc.get('href', '')
            name = bc.get_text(strip=True)
            match = re.search(r'/category/(\d+)/', href)
            if match and len(name) < 30:
                add_category(match.group(1), name)

        # æ–¹æ³•2: ä» genre é“¾æ¥æå– (/genre/) - æ€»æ˜¯æ‰§è¡Œ
        for gl in soup.select('a[href*="/genre/"]'):
            href = gl.get('href', '')
            name = gl.get_text(strip=True)
            match = re.search(r'/genre/(\d+)', href)
            if match and len(name) < 30:
                add_category(match.group(1), name)

        # æ–¹æ³•3: ä» ranking é“¾æ¥æå– - æ€»æ˜¯æ‰§è¡Œ
        for link in soup.select('a[href*="ranking.rakuten.co.jp"]'):
            href = link.get('href', '')
            match = re.search(r'ranking\.rakuten\.co\.jp/\w+/(\d+)', href)
            if match:
                name = link.get_text(strip=True)
                add_category(match.group(1), name)

        # æ–¹æ³•4: ä»é¡µé¢å†…åµŒçš„JSONæ•°æ®æå– (å¤šç§æ ¼å¼) - æ€»æ˜¯æ‰§è¡Œ
        # åŒ¹é… genreId, categoryId, genre_id, category_id ç­‰
        genre_matches = re.findall(r'["\'](?:genre|category)[_]?[Ii]d["\']\s*:\s*["\']?(\d{5,})', page_text)
        for gid in genre_matches[:10]:
            add_category(gid, None)

        # æ–¹æ³•5: ä» JSON-LD æˆ– script æ ‡ç­¾æå– - æ€»æ˜¯æ‰§è¡Œ
        # åŒ¹é… "genreId":"123456" æˆ– genreId=123456 æˆ– "genreId":123456
        all_genres = re.findall(r'genre[Ii]d["\'\s:=]+["\']?(\d{5,})', page_text)
        for gid in all_genres[:10]:
            add_category(gid, None)

        # æ–¹æ³•6: ä» URL å‚æ•°æå– - æ€»æ˜¯æ‰§è¡Œ
        url_genres = re.findall(r'[?&]genre[_]?id=(\d{5,})', page_text, re.I)
        for gid in url_genres[:5]:
            add_category(gid, None)

        # æ–¹æ³•7: ä»å¸¸è§çš„ä¹å¤©ç±»ç›®æ ¼å¼æå–
        # åŒ¹é… "l-id=xxx_yyy_zzzzz" æ ¼å¼ä¸­çš„æ•°å­—ID
        lid_matches = re.findall(r'l-id=[^"\'&]*?(\d{6})', page_text)
        for gid in lid_matches[:5]:
            add_category(gid, None)

        # è°ƒè¯•è¾“å‡º
        if categories:
            print(f"      [ç±»ç›®DEBUG] æå–åˆ° {len(categories)} ä¸ªç±»ç›®: {[c['id'] for c in categories[:5]]}")
        else:
            print(f"      [ç±»ç›®DEBUG] âš ï¸ æœªèƒ½æå–åˆ°ç±»ç›®ä¿¡æ¯")

    except Exception as e:
        print(f"      [ç±»ç›®DEBUG] æå–å¼‚å¸¸: {e}")

    return categories


# ==================== âš™ï¸ æ’è¡Œæ¦œé…ç½® ====================
RANKING_SEARCH_PAGES = 3  # æœç´¢æ·±åº¦


# =======================================================

def search_product_in_ranking(shop_id, item_id, category_id, rank_type="daily"):
    """
    é€šç”¨æ’åæŸ¥æ‰¾å‡½æ•° (HTMLåˆ‡å‰²æ³• - ç»ˆæç²¾å‡†ç‰ˆ)
    é€»è¾‘ï¼š
    1. åœ¨æºç ä¸­æœç´¢â€œXä½â€è¿™ä¸ªæ’åæ ‡è®° (å¦‚ 'alt="1ä½"' æˆ– 'alt="81ä½"')ã€‚
    2. æ‰¾åˆ°åï¼ŒæŠŠè¿™ä¸ªæ ‡è®° **ä¹‹å‰** çš„æ‰€æœ‰ HTML ä»£ç å…¨éƒ¨åˆ‡é™¤ï¼
    3. è¿™æ ·å°±å½»åº•åˆ é™¤äº†é¡¶éƒ¨çš„â€œæµè§ˆè®°å½•â€å’Œâ€œå¹¿å‘Šâ€å¹²æ‰°ã€‚
    4. åœ¨å‰©ä¸‹çš„çº¯å‡€ä»£ç é‡Œæ•°æ•°ï¼Œå•†å“æ’ç¬¬å‡ ï¼Œå°±æ˜¯ç¬¬å‡ åã€‚
    """
    base_url = f"https://ranking.rakuten.co.jp/{rank_type}/{category_id}/"

    # ç›®æ ‡ï¼šåº—é“ºå/å•†å“ID ç»„åˆ (å°å†™)
    target_shop = shop_id.lower()
    target_item = item_id.lower()
    target_full = f"{target_shop}/{target_item}"  # å®Œæ•´åŒ¹é…

    try:
        for page in range(1, RANKING_SEARCH_PAGES + 1):
            url = base_url
            if page > 1: url += f"?p={page}"

            print(f"        ğŸ” [æ­£åœ¨æ£€æŸ¥] p{page}: {url} ...", end="", flush=True)

            resp = requests.get(url, impersonate="chrome120", proxies=proxies, timeout=15)
            if resp.status_code != 200:
                print(" âŒ HTTPé”™è¯¯")
                return None, None

            # è½¬å°å†™ç”¨äºæœç´¢
            full_html = resp.text.lower()

            # 1. è¿‡æ»¤æ— æ•ˆé¡µé¢
            if "ãƒšãƒ¼ã‚¸ãŒè¡¨ç¤ºã§ãã¾ã›ã‚“" in full_html:
                print(" âš ï¸ æ— æ•ˆæ¦œå•ï¼Œè·³è¿‡")
                return None, None

            # 2. æ£€æŸ¥é‡å®šå‘
            if "ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°" in full_html and rank_type not in full_html:
                # ç®€å•æ£€æŸ¥æ ‡é¢˜ï¼Œé˜²æ­¢è¯¯åˆ¤
                soup_check = BeautifulSoup(resp.text, 'html.parser')
                h1 = soup_check.select_one('h1')
                if h1 and "ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°" in h1.get_text():
                    print(" âš ï¸ è½¬è·³ç»¼åˆæ¦œï¼Œè·³è¿‡")
                    return None, None

            # --- âœ‚ï¸ æ ¸å¿ƒé€»è¾‘ï¼šå¯»æ‰¾åˆ‡å…¥ç‚¹ (Anchor) ---

            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°æ¦œå•å¼€å§‹çš„åœ°æ–¹ã€‚ç‰¹å¾é€šå¸¸æ˜¯ "1ä½", "46ä½", "81ä½" ç­‰å›¾ç‰‡æˆ–æ–‡å­—
            # åŒ¹é… alt="æ•°å­—ä½" æˆ–è€… >æ•°å­—ä½<
            # æˆ‘ä»¬å¯»æ‰¾é¡µé¢ä¸Šå‡ºç°çš„ç¬¬ä¸€ä¸ªâ€œæ’åæ•°å­—â€

            start_anchor_rank = 1  # é»˜è®¤ä»1åå¼€å§‹
            slice_index = 0  # é»˜è®¤ä¸åˆ‡å‰²

            # æ­£åˆ™æœç´¢ç¬¬ä¸€ä¸ªå‡ºç°çš„æ’åæ ‡è®°
            # æ¨¡å¼ï¼šalt="123ä½" æˆ– class="rank">123ä½
            match = re.search(r'(?:alt="|class="[^"]*rank[^"]*".*?>|>\s*)(\d{1,3})\s*ä½', full_html)

            if match:
                start_anchor_rank = int(match.group(1))
                slice_index = match.end()
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°æ ‡è®°ï¼Œæ ¹æ®é¡µç ä¼°ç®—èµ·å§‹å€¼ (45 æˆ– 80)
                # print(" [æœªå®šä½] ä½¿ç”¨é»˜è®¤èµ·å§‹", end="")
                if page > 1:
                    # å°è¯•åˆ¤æ–­æ˜¯45è¿˜æ˜¯80æ¨¡å¼ï¼Œé»˜è®¤å…ˆç»™80 (ä¹å¤©ç°åœ¨å¾ˆå¤šæ˜¯80)
                    start_anchor_rank = (page - 1) * 80 + 1

            # --- âœ‚ï¸ æ‰§è¡Œåˆ‡å‰²ï¼ ---
            # åªä¿ç•™ä»æ’åå¼€å§‹ä½ç½®ä¹‹åçš„ HTML
            # è¿™æ ·é¡¶éƒ¨çš„ Historyã€Ads å…¨éƒ¨è¢«æ‰”æ‰äº†
            clean_html = full_html[slice_index:]

            # --- ğŸ”® åœ¨çº¯å‡€ HTML ä¸­æŸ¥æ‰¾å•†å“ ---
            # æå–æ‰€æœ‰å•†å“é“¾æ¥ï¼šåº—é“ºå/å•†å“ID ç»„åˆ
            raw_matches = re.findall(r'item\.rakuten\.co\.jp/([^/]+)/([^/"\?]+)', clean_html)

            # âš ï¸ å…³é”®ä¿®å¤ï¼šå»é‡ï¼æ¯ä¸ªå•†å“åªè®¡æ•°ä¸€æ¬¡ï¼ˆä¿æŒé¡ºåºï¼‰
            seen = set()
            found_ids = []  # å­˜å‚¨ "åº—é“ºå/å•†å“ID" ç»„åˆ
            for shop_found, item_found in raw_matches:
                full_id = f"{shop_found}/{item_found}"
                if full_id not in seen:
                    seen.add(full_id)
                    found_ids.append(full_id)

            if target_full in found_ids:
                final_rank = None
                idx_in_list = found_ids.index(target_full)
                print(f" [idx={idx_in_list},å…±{len(found_ids)}ä¸ª]", end="")

                # æ–¹æ³•1: ä» shopUnit[X] JSONæ•°æ®ä¸­æå–æ’å
                shop_unit_matches = list(re.finditer(r'shopUnit\[(\d+)\]', full_html))
                if shop_unit_matches:
                    # æ‰¾åˆ°ç›®æ ‡å•†å“é“¾æ¥çš„ä½ç½®ï¼ˆåŒ…å«åº—é“ºåï¼‰
                    target_pattern = rf'item\.rakuten\.co\.jp/{re.escape(target_shop)}/{re.escape(target_item)}'
                    target_match = re.search(target_pattern, full_html, re.IGNORECASE)

                    if target_match:
                        target_pos = target_match.start()
                        # æ‰¾æœ€æ¥è¿‘ç›®æ ‡ï¼ˆä¸”åœ¨ç›®æ ‡ä¹‹å‰ï¼‰çš„ shopUnit
                        for m in reversed(shop_unit_matches):
                            if m.start() < target_pos:
                                final_rank = int(m.group(1))
                                print(f" [JSON:shopUnit[{final_rank}]]", end="")
                                break

                # æ–¹æ³•2: è®¡æ•°æ³•ï¼ˆä½¿ç”¨åˆ‡å‰²åçš„åˆ—è¡¨ï¼‰
                if final_rank is None:
                    idx = found_ids.index(target_full)
                    final_rank = start_anchor_rank + idx

                    # âš ï¸ ä¿®æ­£ï¼šæ£€æŸ¥ç›®æ ‡å•†å“æ˜¯å¦åœ¨åˆ‡å‰²ååˆ—è¡¨ä¸­"æå‰"å‡ºç°
                    # å¦‚æœç›®æ ‡å•†å“åœ¨åŸå§‹HTMLä¸­çš„ç¬¬ä¸€æ¬¡å‡ºç°ä½ç½®ï¼Œæ¯”åˆ‡å‰²ååˆ—è¡¨ä¸­çš„ç¬¬ä¸€æ¬¡å‡ºç°ä½ç½®æ›´é å‰
                    # è¯´æ˜æœ‰å•†å“è¢«æ¼æ‰äº†ï¼Œéœ€è¦+1
                    if page == 1 and start_anchor_rank == 1:
                        # æ‰¾ç›®æ ‡åœ¨åˆ‡å‰²å‰HTMLä¸­çš„ä½ç½®ï¼ˆä½¿ç”¨å®Œæ•´çš„åº—é“ºå/å•†å“IDï¼‰
                        target_pattern_full = rf'item\.rakuten\.co\.jp/{re.escape(target_shop)}/{re.escape(target_item)}'
                        target_in_full = re.search(target_pattern_full, full_html)
                        target_in_clean = re.search(target_pattern_full, clean_html)

                        if target_in_full and target_in_clean:
                            # ç”¨æ›´å®½æ¾çš„æ­£åˆ™ï¼ŒåŒ¹é…æ‰€æœ‰å¯èƒ½çš„å•†å“é“¾æ¥æ ¼å¼
                            # åŒ…æ‹¬ item.rakuten.co.jp/åº—é“º/å•†å“ å’Œç¼–ç æ ¼å¼
                            pattern = r'item\.rakuten\.co\.jp/([^/"\s]+)/([^/"\s\?]+)'

                            # è®¡ç®—ç›®æ ‡åœ¨åˆ‡å‰²å‰HTMLä¸­æ˜¯ç¬¬å‡ ä¸ªå‡ºç°çš„å•†å“
                            matches_full = re.findall(pattern, full_html[:target_in_full.start()])
                            matches_clean = re.findall(pattern, clean_html[:target_in_clean.start()])

                            # ç”¨ (åº—é“º, å•†å“) ç»„åˆæ¥å»é‡
                            unique_full = len(set(matches_full))
                            unique_clean = len(set(matches_clean))

                            # ç›´æ¥ç”¨ full è®¡æ•°ä½œä¸ºæ’åï¼ˆæ›´å‡†ç¡®ï¼‰
                            final_rank = unique_full + 1
                            print(f" [full={unique_full}+1={final_rank}]", end="")
                        else:
                            print(f" [è®¡æ•°={final_rank}]", end="")
                    else:
                        print(f" [è®¡æ•°={final_rank}]", end="")

                # è·å–ç±»åç”¨äºè¿”å›
                cat_name = ""
                soup = BeautifulSoup(resp.text, 'html.parser')
                title_el = soup.select_one('h1') or soup.select_one('.title')
                if title_el:
                    cat_name = title_el.get_text(strip=True).replace("ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "").replace("ãƒ‡ã‚¤ãƒªãƒ¼", "").strip()

                print(f" âœ… æ’å: {final_rank}å")
                return final_rank, cat_name
            else:
                print(" âŒ ä¸åœ¨æ­¤é¡µæ¦œå•ä¸­")

    except Exception as e:
        print(f" âš ï¸ æŠ¥é”™: {e}")
        pass

    return None, None


def get_category_ranking(shop_id, item_id, page_categories):
    """
    è·å–æ’åä¸»é€»è¾‘ (å¸¦è°ƒè¯•ä¿¡æ¯)
    """
    result = {"å¤§ç±»æ’å": "", "å°ç±»æ’å": ""}
    if not shop_id or not item_id: return result

    found_ranks = []

    # å»é‡ç±»ç›®ID
    unique_cats = {c['id']: c for c in page_categories}.values()

    print(f"      ğŸš€ å‡†å¤‡æ‰«æ {len(unique_cats)} ä¸ªç±»ç›®...")

    # åªæŸ¥æ—¥æ¦œ (daily) - æ‚¨ç›®å‰çš„é…ç½®
    check_types = ["daily"]

    for cat in unique_cats:
        for r_type in check_types:
            rank, cat_name = search_product_in_ranking(shop_id, item_id, cat["id"], r_type)

            if rank:
                display_name = cat_name if cat_name else cat["name"]
                found_ranks.append({
                    "rank": rank,
                    "name": display_name,
                    "id_len": len(cat["id"]),
                })

    # æ€»ç»“ç»“æœ
    if found_ranks:
        # æ’åºï¼šä¼˜å…ˆå–IDé•¿çš„(å°ç±»)ï¼Œå…¶æ¬¡çœ‹æ’åé å‰çš„
        found_ranks.sort(key=lambda x: (-x["id_len"], x["rank"]))

        best = found_ranks[0]
        result["å°ç±»æ’å"] = f"{best['name']} ç¬¬{best['rank']}"
        print(f"      ğŸ‰ æœ€ç»ˆé”å®š: å°ç±»[{result['å°ç±»æ’å']}]", end="")

        if len(found_ranks) > 1:
            # æ‰¾ä¸€ä¸ªåå­—ä¸ä¸€æ ·çš„å¤§ç±»
            for r in found_ranks[1:]:
                if r["name"] != best["name"]:
                    result["å¤§ç±»æ’å"] = f"{r['name']} ç¬¬{r['rank']}"
                    print(f" / å¤§ç±»[{result['å¤§ç±»æ’å']}]")
                    break

            # å¦‚æœæ²¡æ‰¾åˆ°ä¸ä¸€æ ·åå­—çš„ï¼Œå¡«ç¬¬äºŒä¸ª
            if not result["å¤§ç±»æ’å"]:
                sec = found_ranks[1]
                result["å¤§ç±»æ’å"] = f"{sec['name']} ç¬¬{sec['rank']}"
                print(f" / å¤§ç±»[{result['å¤§ç±»æ’å']}]")
        else:
            print("")  # æ¢è¡Œ
    else:
        print("      ğŸ¤·â€â™‚ï¸ æœªåœ¨ä»»ä½•æ¦œå•æ‰¾åˆ°æ’å")

    return result


def extract_ranking_info(soup, raw_html, shop_id="", item_id=""):
    """ä»è¯¦æƒ…é¡µæå–å¤§ç±»æ’åå’Œå°ç±»æ’åï¼Œæ”¯æŒæ’è¡Œæ¦œåæŸ¥"""
    result = {"å¤§ç±»æ’å": "", "å°ç±»æ’å": ""}

    try:
        page_text = soup.get_text()

        # æ–¹æ³•1: åŒ¹é…æ’åå¾½ç« æ–‡æœ¬ "XXXãƒ©ãƒ³ã‚­ãƒ³ã‚° Xä½"
        rank_patterns = re.findall(r'([^\s]{2,20}ãƒ©ãƒ³ã‚­ãƒ³ã‚°)[^\d]*(\d{1,4})ä½', page_text)

        for category, rank in rank_patterns:
            rank_text = f"{category} {rank}ä½"
            if any(kw in category for kw in ["ãƒ‡ã‚¤ãƒªãƒ¼", "é€±é–“", "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ "]):
                if not result["å°ç±»æ’å"]:
                    result["å°ç±»æ’å"] = rank_text
            else:
                if not result["å¤§ç±»æ’å"]:
                    result["å¤§ç±»æ’å"] = rank_text

        # æ–¹æ³•2: ä»HTMLå±æ€§/classä¸­æŸ¥æ‰¾æ’åå…ƒç´ 
        rank_elements = soup.select('[class*="rank"]') or soup.select('[class*="Rank"]')
        for el in rank_elements:
            text = el.get_text(strip=True)
            match = re.search(r'(\d{1,4})ä½', text)
            if match:
                rank_num = match.group(1)
                if not result["å°ç±»æ’å"]:
                    result["å°ç±»æ’å"] = f"æ’å {rank_num}ä½"
                    break

        # æ–¹æ³•3: ä»ä¹å¤©æ’è¡Œæ¦œé¡µé¢åæŸ¥ (è·å– "ç±»ç›®å ç¬¬X" æ ¼å¼) - ä¼˜å…ˆçº§æœ€é«˜ï¼
        if shop_id and item_id:
            # è‡ªåŠ¨ä»é¡µé¢æå–ç±»ç›®ID
            page_categories = get_categories_from_page(soup, raw_html)
            if page_categories:
                print(f"      â†’ æ’è¡Œæ¦œåæŸ¥ä¸­ (å‘ç°{len(page_categories)}ä¸ªç±»ç›®)...")
                ranking_data = get_category_ranking(shop_id, item_id, page_categories)
                # åæŸ¥ç»“æœä¼˜å…ˆè¦†ç›–ï¼ˆæ›´å‡†ç¡®ï¼‰
                if ranking_data["å¤§ç±»æ’å"]:
                    result["å¤§ç±»æ’å"] = ranking_data["å¤§ç±»æ’å"]
                if ranking_data["å°ç±»æ’å"]:
                    result["å°ç±»æ’å"] = ranking_data["å°ç±»æ’å"]

    except Exception as e:
        print(f"      [æ’å] æå–å¤±è´¥: {e}")

    return result


def extract_selling_points(soup):
    """æå–æ ¸å¿ƒå–ç‚¹ (Catch Copy + å•†å“ç®€ä»‹)"""
    points = []
    # 1. Catch Copy (é€šå¸¸åœ¨æ ‡é¢˜ä¸Šæ–¹çš„çº¢è‰²/åŠ ç²—æ–‡å­—)
    catch_copy = soup.select_one('.catch_copy')
    if catch_copy:
        points.append(catch_copy.get_text(strip=True))

    # 2. å•†å“æè¿°çš„å‰å‡ é¡¹ (é€šå¸¸æ˜¯æ ¸å¿ƒå–ç‚¹)
    lis = soup.select('.item_desc li')
    for li in lis[:3]:
        points.append(li.get_text(strip=True))

    if not points:
        # å¤‡é€‰ï¼šæ‰¾ä¸»è¦æè¿°æ®µè½
        desc = soup.select_one('.item_desc')
        if desc: points.append(desc.get_text(strip=True)[:100])

    return "\n".join(points) if points else "æ— æ˜æ˜¾å–ç‚¹"


def analyze_selling_points_ai(raw_points, title=""):
    """
    ã€AIæ ¸å¿ƒå–ç‚¹åˆ†æã€‘ä½¿ç”¨AIå¯¹æå–çš„æ—¥æ–‡å–ç‚¹è¿›è¡Œåˆ†ææ€»ç»“
    è¾“å‡ºç®€æ´çš„ä¸­æ–‡å–ç‚¹æè¿°
    """
    global ai_client
    if not ai_client or not USE_AI_FEATURES:
        return raw_points

    if not raw_points or raw_points == "æ— æ˜æ˜¾å–ç‚¹":
        return raw_points

    prompt = f"""è¯·åˆ†æä»¥ä¸‹æ—¥æœ¬ä¹å¤©å•†å“çš„æ ¸å¿ƒå–ç‚¹ï¼Œç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“å‡º3-5ä¸ªæ ¸å¿ƒå–ç‚¹ã€‚

å•†å“æ ‡é¢˜ï¼š{title[:100]}
åŸå§‹å–ç‚¹æè¿°ï¼š
{raw_points[:800]}

è¦æ±‚ï¼š
1. æ¯ä¸ªå–ç‚¹ç”¨ç®€çŸ­ä¸­æ–‡æè¿°ï¼ˆ10-20å­—ï¼‰
2. æå–æœ€æœ‰ä»·å€¼çš„å–ç‚¹ä¿¡æ¯ï¼ˆå“ç‰Œä¼˜åŠ¿ã€äº§å“ç‰¹ç‚¹ã€ä½¿ç”¨åœºæ™¯ç­‰ï¼‰
3. ç”¨æ¢è¡Œåˆ†éš”æ¯ä¸ªå–ç‚¹

è¯·ç›´æ¥è¾“å‡ºå–ç‚¹åˆ—è¡¨ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

    try:
        response = ai_client.chat.completions.create(
            model=AI_MODEL,
            temperature=0,
            max_tokens=300,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯äº§å“å–ç‚¹åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ—¥æ–‡å•†å“æè¿°ä¸­æå–æ ¸å¿ƒå–ç‚¹å¹¶ç¿»è¯‘æˆç®€æ´ä¸­æ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        result = response.choices[0].message.content.strip()
        if result:
            print(f"      [AIå–ç‚¹] åˆ†æå®Œæˆ")
            return result
    except Exception as e:
        print(f"      [AIå–ç‚¹] åˆ†æå¤±è´¥: {e}")

    return raw_points  # å¤±è´¥æ—¶è¿”å›åŸæ–‡


def summarize_reviews_ai(pros_list, cons_list):
    """
    ã€AIè¯„è®ºæ€»ç»“ã€‘å°†è¯„è®ºæç‚¼æˆç®€çŸ­å…³é”®è¯
    è¾“å‡ºæ ¼å¼ï¼š
    - è¯„è®ºå‡ºç°ä¼˜ç‚¹ï¼šç»„è£…ç®€å•ï¼ˆç®€çŸ­å…³é”®è¯ï¼‰
    - å®¢è¯‰ç‚¹ï¼š1.æ¶å­å˜å½¢ 2.åŒ…è£…ç ´æŸï¼ˆé—®é¢˜åˆ—è¡¨ï¼‰
    """
    global ai_client
    res = {
        "è¯„è®ºå‡ºç°ä¼˜ç‚¹": "æš‚æ— è¯„è®º",
        "å®¢è¯‰ç‚¹": "æ— æ˜æ˜¾å·®è¯„"
    }

    if not ai_client or not USE_AI_FEATURES:
        # æ— AIæ—¶ï¼Œç®€å•æˆªå–
        if pros_list:
            res["è¯„è®ºå‡ºç°ä¼˜ç‚¹"] = pros_list[0][:50] if pros_list else "æš‚æ— è¯„è®º"
        if cons_list:
            res["å®¢è¯‰ç‚¹"] = "\n".join([f"{i + 1}.{c[:30]}" for i, c in enumerate(cons_list[:3])])
        return res

    # å‡†å¤‡è¯„è®ºæ–‡æœ¬ - é‡‡æ ·æ›´å¤šè¯„è®ºä»¥è·å¾—æ›´å‡†ç¡®çš„åˆ†æ
    # ä»å¥½è¯„ä¸­å‡åŒ€é‡‡æ ·30æ¡
    import random
    if len(pros_list) > 30:
        step = len(pros_list) // 30
        sampled_pros = [pros_list[i] for i in range(0, len(pros_list), step)][:30]
    else:
        sampled_pros = pros_list
    pros_text = "\n".join(sampled_pros)

    # å·®è¯„å…¨éƒ¨ä½¿ç”¨ï¼ˆé€šå¸¸æ•°é‡ä¸å¤šï¼‰
    cons_text = "\n".join(cons_list[:15]) if cons_list else "æ— "

    prompt = f"""è¯·åˆ†æä»¥ä¸‹æ—¥æœ¬ä¹å¤©å•†å“çš„ç”¨æˆ·è¯„è®ºï¼Œæç‚¼è¯¥å•†å“ç‹¬ç‰¹çš„è¯„è®ºç‰¹ç‚¹ã€‚

ã€å¥½è¯„å†…å®¹ã€‘ï¼ˆå…±{len(pros_list)}æ¡ï¼Œä»¥ä¸‹ä¸ºé‡‡æ ·{len(sampled_pros)}æ¡ï¼‰:
{pros_text[:3000]}

ã€å·®è¯„/å®¢è¯‰ã€‘ï¼ˆ{len(cons_list)}æ¡ï¼‰:
{cons_text[:1500]}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆå¿…é¡»ç”¨ä¸­æ–‡ï¼‰ï¼š

è¯„è®ºå‡ºç°ä¼˜ç‚¹ï¼š[åˆ—å‡º3-5ä¸ªè¯¥å•†å“è¢«ç”¨æˆ·æåˆ°æœ€å¤šçš„å…·ä½“ä¼˜ç‚¹ï¼Œè¦ä½“ç°å·®å¼‚åŒ–]
- é¿å…æ³›æ³›çš„è¯å¦‚"æ€§ä»·æ¯”é«˜ã€è´¨é‡å¥½"
- ä¼˜å…ˆæå–å…·ä½“ç‰¹ç‚¹ï¼Œå¦‚"é˜²é”ˆå¥½ã€æ‰¿é‡å¼ºã€æ£šæ¿åšå®ã€é¢œè‰²æ¼‚äº®ã€é…ä»¶é½å…¨ã€å®¢æœå¥½"
- ç”¨ç©ºæ ¼åˆ†éš”

å®¢è¯‰ç‚¹ï¼š[åˆ—å‡º1-3ä¸ªç”¨æˆ·æŠ±æ€¨çš„å…·ä½“é—®é¢˜ï¼Œå¦‚"1.èºä¸ç”Ÿé”ˆ 2.æ£šæ¿æœ‰åˆ®ç—• 3.è¯´æ˜ä¹¦éš¾æ‡‚"ï¼Œæ²¡æœ‰å·®è¯„å†™"æ— æ˜æ˜¾å·®è¯„"]

æ³¨æ„ï¼šæå–è¯„è®ºä¸­å‡ºç°é¢‘ç‡é«˜çš„å…·ä½“æè¿°ï¼Œé¿å…ç¬¼ç»Ÿè¯æ±‡ã€‚"""

    try:
        response = ai_client.chat.completions.create(
            model=AI_MODEL,
            temperature=0,
            max_tokens=200,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯è¯„è®ºåˆ†æå¸ˆï¼Œæ“…é•¿ä»æ—¥æ–‡è¯„è®ºä¸­æç‚¼å…³é”®ä¿¡æ¯ï¼Œè¾“å‡ºç®€æ´ä¸­æ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        result = response.choices[0].message.content.strip()

        # è§£æAIè¾“å‡º
        if "è¯„è®ºå‡ºç°ä¼˜ç‚¹" in result or "ä¼˜ç‚¹" in result:
            # æå–ä¼˜ç‚¹
            import re
            pros_match = re.search(r'(?:è¯„è®ºå‡ºç°)?ä¼˜ç‚¹[ï¼š:]\s*(.+?)(?:\n|å®¢è¯‰|$)', result)
            if pros_match:
                res["è¯„è®ºå‡ºç°ä¼˜ç‚¹"] = pros_match.group(1).strip()[:50]

            # æå–å®¢è¯‰ç‚¹
            cons_match = re.search(r'å®¢è¯‰ç‚¹[ï¼š:]\s*(.+?)$', result, re.DOTALL)
            if cons_match:
                res["å®¢è¯‰ç‚¹"] = cons_match.group(1).strip()[:100]

        print(f"      [AIè¯„è®º] æ€»ç»“å®Œæˆ: ä¼˜ç‚¹={res['è¯„è®ºå‡ºç°ä¼˜ç‚¹'][:15]}...")
        return res

    except Exception as e:
        print(f"      [AIè¯„è®º] æ€»ç»“å¤±è´¥: {e}")
        # å¤±è´¥æ—¶ç”¨ç®€å•æ–¹å¼
        if pros_list:
            res["è¯„è®ºå‡ºç°ä¼˜ç‚¹"] = pros_list[0][:50]
        if cons_list:
            res["å®¢è¯‰ç‚¹"] = "\n".join([f"{i + 1}.{c[:30]}" for i, c in enumerate(cons_list[:3])])
        return res


def analyze_reviews(review_url, max_pages=10):
    """
    ã€æ–°å¢ã€‘è¿›å…¥è¯„è®ºé¡µï¼Œåˆ†æä¼˜ç¼ºç‚¹å’Œå®¢è¯‰
    è‡ªåŠ¨æ£€æµ‹æ€»é¡µæ•°å¹¶æŠ“å–æ‰€æœ‰è¯„è®º
    """
    res = {
        "è¯„è®ºå‡ºç°ä¼˜ç‚¹": "æš‚æ— è¯„è®º",
        "å®¢è¯‰ç‚¹": "æ— æ˜æ˜¾å·®è¯„"
    }
    if not review_url or "http" not in review_url: return res

    try:
        pros = []  # 4-5æ˜Ÿ
        cons = []  # 1-2æ˜Ÿ (å®¢è¯‰)

        # æ„å»ºåŸºç¡€URLï¼ˆå»æ‰æœ«å°¾çš„æ’åºå‚æ•°ï¼‰
        # è¯„è®ºURLæ ¼å¼: https://review.rakuten.co.jp/item/1/306224_10008717/1.1/
        # åˆ†é¡µæ ¼å¼: https://review.rakuten.co.jp/item/1/306224_10008717/?p=2
        base_url = re.sub(r'/\d+\.\d+/?$', '/', review_url)
        print(f"      [è¯„è®ºDEBUG] base_url: {base_url}")

        # å…ˆè·å–ç¬¬ä¸€é¡µï¼Œæ£€æµ‹æ€»é¡µæ•°ï¼ˆç”¨åŸºç¡€URLï¼Œä¸å¸¦æ’åºå‚æ•°ï¼‰
        first_resp = requests.get(base_url, impersonate="chrome120", proxies=proxies, timeout=10)

        # ä»é¡µé¢ä¸­æå–æ€»é¡µæ•°ï¼ˆå¤šç§æ–¹å¼ï¼‰
        # æ–¹å¼1: åŒ¹é… ?p=X çš„æœ€å¤§å€¼
        page_nums = re.findall(r'\?p=(\d+)', first_resp.text)
        # æ–¹å¼2: åŒ¹é… "Xé¡µä¸­" æˆ– "å…¨Xé¡µ"
        if not page_nums:
            total_match = re.search(r'å…¨(\d+)ãƒšãƒ¼ã‚¸|(\d+)ãƒšãƒ¼ã‚¸ä¸­', first_resp.text)
            if total_match:
                page_nums = [total_match.group(1) or total_match.group(2)]
        # æ–¹å¼3: ä»è¯„è®ºæ€»æ•°è®¡ç®—ï¼ˆæ¯é¡µçº¦30æ¡ï¼‰
        if not page_nums:
            total_count_match = re.search(r'"reviewCount"\s*:\s*"?(\d+)"?', first_resp.text)
            if total_count_match:
                total_count = int(total_count_match.group(1))
                calculated_pages = (total_count + 29) // 30  # æ¯é¡µçº¦30æ¡ï¼Œå‘ä¸Šå–æ•´
                page_nums = [str(calculated_pages)]
                print(f"      [è¯„è®ºDEBUG] ä»è¯„è®ºæ€»æ•°{total_count}è®¡ç®—å¾—{calculated_pages}é¡µ")

        if page_nums:
            total_pages = min(int(max(page_nums, key=int)), max_pages)  # æœ€å¤šæŠ“max_pagesé¡µ
        else:
            total_pages = 1
        print(f"      [è¯„è®º] æ£€æµ‹åˆ° {total_pages} é¡µè¯„è®ºï¼Œå¼€å§‹æŠ“å–...")

        # æŠ“å–æ‰€æœ‰é¡µè¯„è®º
        total_reviews = 0
        for page in range(1, total_pages + 1):
            page_url = f"{base_url}?p={page}" if page > 1 else review_url
            try:
                resp = requests.get(page_url, impersonate="chrome120", proxies=proxies, timeout=10)
                page_text = resp.text

                # ä¹å¤©2024æ–°ç‰ˆè¯„è®ºæ ¼å¼ï¼šè¯„è®ºæ–‡æœ¬åœ¨ </div></div><div class="expand-link ä¹‹å‰
                review_pattern = r'>([^<]{20,500})</div></div><div class="expand-link'
                matches = re.findall(review_pattern, page_text)

                if not matches:
                    break  # æ²¡æœ‰æ›´å¤šè¯„è®ºäº†

                # æå–è¯„è®ºæ–‡æœ¬
                for text in matches:
                    text = text.strip().replace('\n', ' ')[:200]
                    if len(text) > 15 and not text.startswith('<'):
                        # ç®€å•åˆ¤æ–­å¥½è¯„/å·®è¯„ï¼ˆæ ¹æ®å…³é”®è¯ï¼‰
                        negative_words = ['æ®‹å¿µ', 'æ‚ªã„', 'ãŒã£ã‹ã‚Š', 'æœ€æ‚ª', 'å£Šã‚Œ', 'å‚·', 'ãƒ€ãƒ¡', 'ä¸è‰¯', 'æ¬ ã‘', 'å‡¹', 'éŒ†']
                        if any(w in text for w in negative_words):
                            cons.append(text)
                        else:
                            pros.append(text)
                        total_reviews += 1

                time.sleep(0.3)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"      [è¯„è®º] ç¬¬{page}é¡µæŠ“å–å¤±è´¥: {e}")
                break

        print(f"      [è¯„è®ºDEBUG] å…±æŠ“å– {total_pages} é¡µ, æ‰¾åˆ° {total_reviews} æ¡è¯„è®º (å¥½è¯„{len(pros)}/å·®è¯„{len(cons)})")

        # ç”¨AIæ€»ç»“è¯„è®ºï¼ˆç®€çŸ­å…³é”®è¯æ ¼å¼ï¼‰
        if pros or cons:
            res = summarize_reviews_ai(pros, cons)

        return res
    except Exception as e:
        print(f"      [è¯„è®ºåˆ†æ] é”™è¯¯: {e}")
        return res


# ==============================================================


def get_product_details(item_url, review_count_check=1):
    """
    æ·±åº¦æŠ“å–ä¸»é€»è¾‘ï¼šæ•´åˆäº† ä¸Šçº¿æ—¶é—´ + å‚æ•° + å–ç‚¹ + è§†é¢‘ + è¯„è®ºåˆ†æ
    """
    result = {
        "ä¸Šçº¿æ—¶é•¿": "ç­‰å¾…æŠ“å–...",
        "å•†å“è¯¦ç»†å‚æ•°": "ç­‰å¾…æŠ“å–...",
        "æ ¸å¿ƒå–ç‚¹åˆ†æ": "...",
        "è¯„è®ºå‡ºç°ä¼˜ç‚¹": "...",
        "å®¢è¯‰ç‚¹": "...",
        "æœ‰æ— è§†é¢‘": "æ— ",
        "å¤§ç±»æ’å": "",
        "å°ç±»æ’å": "",
        "ä¸»å›¾": "",  # ä»è¯¦æƒ…é¡µè·å–é«˜æ¸…ä¸»å›¾
        "å¤‡æ³¨": None,
        "è¯„è®ºæ•°_è¡¥å……": None,  # ä»è¯¦æƒ…é¡µè¡¥å……çš„è¯„è®ºæ•°
        "è¯„åˆ†_è¡¥å……": None,  # ä»è¯¦æƒ…é¡µè¡¥å……çš„è¯„åˆ†
        "äº§å“ç‰¹å¾": [],  # AIæå–çš„äº§å“ç‰¹å¾åˆ—è¡¨
    }

    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤‡ç”¨è¯„è®ºé“¾æ¥ (æ ¼å¼: åŸå§‹é“¾æ¥|||è¯„è®ºé“¾æ¥)
        review_backup = ""
        if "|||" in item_url:
            parts = item_url.split("|||")
            item_url = parts[0]
            review_backup = parts[1] if len(parts) > 1 else ""

        # 1. è§£æçœŸå® URL
        real_url = resolve_pr_link(item_url)
        real_url = unquote(real_url)
        print(f"      [DEBUG] è§£æåURL: {real_url[:80]}...")

        m = re.search(r'item\.rakuten\.co\.jp/([^/?#]+)/([^/?#]+)', real_url)
        if not m:
            # å°è¯•å¤‡ç”¨è§£æï¼šä» product.rakuten.co.jp æ ¼å¼
            m = re.search(r'product\.rakuten\.co\.jp/product/-/([^/?#]+)', real_url)
            if m:
                # è¿™æ˜¯äº§å“èšåˆé¡µï¼Œå°è¯•ç›´æ¥è®¿é—®è·å–å•†å“é“¾æ¥
                print(f"      [DEBUG] æ£€æµ‹åˆ°äº§å“èšåˆé¡µï¼Œå°è¯•è®¿é—®...")
                try:
                    prod_resp = requests.get(real_url, impersonate="chrome120", proxies=proxies, timeout=15)
                    m2 = re.search(r'item\.rakuten\.co\.jp/([^/?#"]+)/([^/?#"]+)', prod_resp.text)
                    if m2:
                        m = m2
                except:
                    pass

        # ğŸ”¥ å¤‡ç”¨æ–¹æ¡ˆï¼šä»è¯„è®ºé¡µè·å–å•†å“é“¾æ¥
        if not m and review_backup:
            print(f"      [DEBUG] å°è¯•ä»è¯„è®ºé¡µè·å–å•†å“é“¾æ¥: {review_backup[:60]}...")
            try:
                review_resp = requests.get(review_backup, impersonate="chrome120", proxies=proxies, timeout=15)
                # ä»è¯„è®ºé¡µHTMLä¸­æå–å•†å“é“¾æ¥
                m = re.search(r'item\.rakuten\.co\.jp/([^/?#"\']+)/([^/?#"\']+)', review_resp.text)
                if m:
                    print(f"      [DEBUG] âœ… ä»è¯„è®ºé¡µæˆåŠŸæå–å•†å“é“¾æ¥!")
            except Exception as e:
                print(f"      [DEBUG] è¯„è®ºé¡µè®¿é—®å¤±è´¥: {e}")

        if not m:
            result["ä¸Šçº¿æ—¶é•¿"] = "URLè§£æå¤±è´¥"
            result["å¤‡æ³¨"] = f"æ— æ³•è§£æURL: {real_url[:50]}"
            return result

        shop_id, item_id = m.group(1), m.group(2)
        item_page_url = f"https://item.rakuten.co.jp/{shop_id}/{item_id}/"
        print(f"      [DEBUG] å•†å“é¡µURL: {item_page_url}")

        # 2. è¯·æ±‚å•†å“è¯¦æƒ…é¡µ
        item_resp = requests.get(item_page_url, impersonate="chrome120", proxies=proxies, timeout=15)
        if item_resp.status_code != 200: return result

        item_soup = BeautifulSoup(item_resp.text, 'html.parser')

        # --- A. æŠ“å–å‚æ•° ---
        result["å•†å“è¯¦ç»†å‚æ•°"] = extract_dynamic_specs(item_soup)

        # --- âœ¨ æ–°å¢: ä»è¯¦æƒ…é¡µè¡¥å……è¯„è®ºæ•°å’Œè¯„åˆ† ---
        # ç›´æ¥ä»HTMLä¸­ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™åŒ¹é…ä¹å¤©è¯„åˆ†æ ¼å¼
        # å…¸å‹æ ¼å¼: "4.63(1,526ä»¶)" æˆ–åœ¨JSONä¸­ "reviewAverage":"4.63","reviewCount":1526

        # æ–¹æ³•1: ä»JSON-LDæˆ–è„šæœ¬ä¸­æå–
        json_score = re.search(r'"ratingValue"\s*:\s*"?(\d\.\d+)"?', item_resp.text)
        json_count = re.search(r'"reviewCount"\s*:\s*"?(\d+)"?', item_resp.text)
        if json_score and json_count:
            result["è¯„åˆ†_è¡¥å……"] = json_score.group(1)
            result["è¯„è®ºæ•°_è¡¥å……"] = int(json_count.group(1))

        # æ–¹æ³•2: ä»é¡µé¢æ–‡æœ¬åŒ¹é… 4.63(1,526ä»¶)
        if result["è¯„åˆ†_è¡¥å……"] is None:
            review_match = re.search(r'(\d\.\d{1,2})\s*[\(ï¼ˆ]([0-9,]+)\s*ä»¶[\)ï¼‰]', item_resp.text)
            if review_match:
                result["è¯„åˆ†_è¡¥å……"] = review_match.group(1)
                result["è¯„è®ºæ•°_è¡¥å……"] = int(review_match.group(2).replace(',', ''))

        # æ–¹æ³•3: åˆ†å¼€åŒ¹é…
        if result["è¯„åˆ†_è¡¥å……"] is None:
            score_m = re.search(r'>(\d\.\d{1,2})<', item_resp.text)  # æ ‡ç­¾å†…çš„è¯„åˆ†
            cnt_m = re.search(r'[\(ï¼ˆ]([0-9,]+)\s*ä»¶[\)ï¼‰]', item_resp.text)
            if score_m and cnt_m:
                score_val = float(score_m.group(1))
                if 1.0 <= score_val <= 5.0:
                    result["è¯„åˆ†_è¡¥å……"] = score_m.group(1)
                    result["è¯„è®ºæ•°_è¡¥å……"] = int(cnt_m.group(1).replace(',', ''))

        # --- âœ¨ æ–°å¢: æŠ“å–é«˜æ¸…ä¸»å›¾ ---
        main_img = ""
        # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–ä¸»å›¾
        img_el = (item_soup.select_one('meta[property="og:image"]') or
                  item_soup.select_one('.rakutenLimitedId_ImageMain1-3 img') or
                  item_soup.select_one('.image-main img') or
                  item_soup.select_one('[class*="mainImage"] img') or
                  item_soup.select_one('.item-image img'))
        if img_el:
            main_img = img_el.get('content') or img_el.get('src') or ""
        # å¤‡ç”¨ï¼šä»é¡µé¢æºç æ­£åˆ™æå–
        if not main_img:
            img_match = re.search(r'"image"\s*:\s*"(https://[^"]+\.(?:jpg|jpeg|png|webp))"', item_resp.text, re.I)
            if img_match:
                main_img = img_match.group(1)
        result["ä¸»å›¾"] = main_img

        # --- âœ¨ æ–°å¢: æŠ“å–æ ¸å¿ƒå–ç‚¹ ---
        raw_selling_points = extract_selling_points(item_soup)

        # --- âœ¨ æ–°å¢: AIæå–äº§å“ç‰¹å¾ ---
        # è·å–æ ‡é¢˜
        title_tag = item_soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else ""

        # ç”¨AIåˆ†ææ ¸å¿ƒå–ç‚¹ï¼ˆç¿»è¯‘+æ€»ç»“ï¼‰
        result["æ ¸å¿ƒå–ç‚¹åˆ†æ"] = analyze_selling_points_ai(raw_selling_points, title)
        # è·å–æè¿°
        desc_tag = item_soup.select_one('.item_desc') or item_soup.select_one('[class*="description"]')
        description = desc_tag.get_text(strip=True)[:500] if desc_tag else ""

        # æå–è¯¦æƒ…é¡µä¸­çš„äº§å“å±•ç¤ºå›¾ç‰‡ï¼ˆç”¨äºAIè§†è§‰åˆ†æï¼‰
        detail_images = []
        if USE_AI_FEATURES:
            # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨å•†å“ä¸»å›¾ï¼ˆæœ€å‡†ç¡®ï¼‰
            if main_img and 'http' in main_img:
                detail_images.append(main_img)

            # ä»å•†å“æè¿°åŒºåŸŸæå–å›¾ç‰‡ï¼ˆä¸¥æ ¼è¿‡æ»¤ï¼Œæ’é™¤å¹¿å‘ŠåŒºåŸŸï¼‰
            # åªé€‰æ‹©å•†å“æè¿°åŒºåŸŸçš„å›¾ç‰‡ï¼Œä¸è¦ç”¨ .rakutenLimitedIdï¼ˆå¤ªå®½æ³›ä¼šåŒ…å«å¹¿å‘Šï¼‰
            desc_imgs = item_soup.select('.item_desc img, .item-image img, [class*="itemImage"] img')
            for img in desc_imgs[:8]:
                src = img.get('src') or img.get('data-src') or ''
                if src and 'http' in src and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    # ä¸¥æ ¼è¿‡æ»¤ï¼šæ’é™¤å¹¿å‘Šã€ä¿ƒé”€ã€å­£èŠ‚æ´»åŠ¨å›¾
                    skip_words = ['icon', 'logo', 'banner', 'campaign', 'sale', 'point', 'review',
                                  'cart', 'btn', 'button', 'arrow', 'star', 'rank', 'pr_', 'ad_',
                                  'winter', 'summer', 'spring', 'autumn', 'season', 'event', 'special',
                                  'entry', 'coupon', 'deal', 'stamp', 'subscription', 'å®šæœŸè³¼å…¥',
                                  'snowman', 'tea', 'coffee', 'hand', 'warm', 'tokushu', 'guide']
                    if any(w in src.lower() for w in skip_words):
                        continue
                    # åªä¿ç•™çœŸæ­£çš„å•†å“å›¾ç‰‡
                    if ('image.rakuten' in src or 'shop.r10s' in src or 'thumbnail' in src) and shop_id in src:
                        if src not in detail_images:
                            detail_images.append(src)

            # é™åˆ¶æœ€å¤š5å¼ å›¾ç‰‡
            detail_images = detail_images[:5]
            if detail_images:
                print(f"      [AI] å‘ç° {len(detail_images)} å¼ å•†å“å›¾å¾…åˆ†æ")

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ä¿¡æ¯ç”¨äºç‰¹å¾æå–ï¼ˆåŒ…å«æ ¸å¿ƒå–ç‚¹ï¼‰
        full_description = f"{description}\n{result['æ ¸å¿ƒå–ç‚¹åˆ†æ']}"

        # æå–ç‰¹å¾ï¼ˆæ”¯æŒå›¾ç‰‡åˆ†æï¼‰
        result["äº§å“ç‰¹å¾"] = extract_product_features(title, full_description, result["å•†å“è¯¦ç»†å‚æ•°"], detail_images)

        # --- âœ¨ æ–°å¢: æŠ“å–æœ‰æ— è§†é¢‘ ---
        result["æœ‰æ— è§†é¢‘"] = check_has_video(item_soup, item_resp.text)

        # --- âœ¨ æ–°å¢: æŠ“å–æ’åä¿¡æ¯ (æ”¯æŒæ’è¡Œæ¦œåæŸ¥) ---
        print(f"      [æ’åDEBUG] æ­£åœ¨æŸ¥è¯¢: shop={shop_id}, item={item_id}")
        ranking_info = extract_ranking_info(item_soup, item_resp.text, shop_id, item_id)
        result["å¤§ç±»æ’å"] = ranking_info["å¤§ç±»æ’å"]
        result["å°ç±»æ’å"] = ranking_info["å°ç±»æ’å"]
        # ä¿å­˜ç¿»è¯‘åçš„ç‰ˆæœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        result["å¤§ç±»æ’å_CN"] = ranking_info.get("å¤§ç±»æ’å_CN", "")
        result["å°ç±»æ’å_CN"] = ranking_info.get("å°ç±»æ’å_CN", "")

        # --- B. æŠ“å–ä¸Šçº¿æ—¶é—´ ä¸ è¯„è®ºåˆ†æ ---
        # å¦‚æœåˆ—è¡¨é¡µè¯„è®ºæ•°æ˜¯0ï¼Œä½†è¯¦æƒ…é¡µå¯èƒ½æœ‰è¯„è®ºï¼Œç”¨è¯¦æƒ…é¡µçš„æ•°æ®é‡æ–°åˆ¤æ–­
        actual_review_count = review_count_check
        if int(review_count_check) == 0 and result.get("è¯„è®ºæ•°_è¡¥å……"):
            actual_review_count = result["è¯„è®ºæ•°_è¡¥å……"]
            print(f"      [DEBUG] åˆ—è¡¨é¡µè¯„è®ºæ•°0ï¼Œä½†è¯¦æƒ…é¡µæœ‰è¯„è®º: {actual_review_count}")

        if int(actual_review_count) == 0:
            result["ä¸Šçº¿æ—¶é•¿"] = "æš‚æ— è¯„è®º(æ–°å“)"
            result["è¯„è®ºå‡ºç°ä¼˜ç‚¹"] = "æ— è¯„è®º"
            result["å®¢è¯‰ç‚¹"] = "æ— è¯„è®º"
        else:
            # åœ¨è¯¦æƒ…é¡µé‡Œæ‰¾è¯„è®ºé“¾æ¥
            review_match = re.search(r'href="(https://review\.rakuten\.co\.jp/item/1/[^"]+)"', item_resp.text)
            if review_match:
                review_url = review_match.group(1)

                # --- âœ¨ æ–°å¢: æ·±åº¦åˆ†æè¯„è®ºå†…å®¹ (ä¼˜ç‚¹/å®¢è¯‰) ---
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å•ç‹¬è¯·æ±‚ä¸€æ¬¡è¯„è®ºé¡µï¼Œæ—¢ä¸ºäº†æ‹¿æ—¶é—´ï¼Œä¹Ÿä¸ºäº†æ‹¿å†…å®¹
                # ä¸ºäº†ä»£ç å¤ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ analyze_reviews é‡Œæ‹¿å†…å®¹ï¼Œåœ¨è¿™é‡Œå•ç‹¬æ‹¿æ—¶é—´ï¼Œæˆ–è€…åˆå¹¶
                # è¿™é‡Œä¸ºäº†ç¨³å¥ï¼Œåˆ†åˆ«å¤„ç†

                # 1. è·å–ä¼˜ç¼ºç‚¹
                print(f"      [DEBUG] æ­£åœ¨åˆ†æè¯„è®º: {review_url}")
                review_data = analyze_reviews(review_url)
                result["è¯„è®ºå‡ºç°ä¼˜ç‚¹"] = review_data["è¯„è®ºå‡ºç°ä¼˜ç‚¹"]
                result["å®¢è¯‰ç‚¹"] = review_data["å®¢è¯‰ç‚¹"]
                print(f"      [DEBUG] è¯„è®ºç»“æœ: ä¼˜ç‚¹={review_data['è¯„è®ºå‡ºç°ä¼˜ç‚¹'][:30]}...")

                time.sleep(0.5)

                # 2. è·å–ä¸Šçº¿æ—¶é—´ - ç›´æ¥è®¿é—®æŒ‰æ—¶é—´ä»æ—§åˆ°æ–°æ’åºçš„è¯„è®ºé¡µ
                # è¯„è®ºURLæ ¼å¼: https://review.rakuten.co.jp/item/1/306224_10008717/1.1/
                # éœ€è¦å»æ‰æœ«å°¾çš„ /x.x/ éƒ¨åˆ†
                base_review_url = review_url.split('?')[0]
                base_review_url = re.sub(r'/\d+\.\d+/?$', '', base_review_url)

                # å…ˆè®¿é—®ç¬¬ä¸€é¡µè·å–æ€»è¯„è®ºæ•°å’Œè®¡ç®—æœ€åä¸€é¡µ
                rev_resp = requests.get(base_review_url, impersonate="chrome120", proxies=proxies, timeout=15)

                # ä»JSONä¸­æå–æ€»è¯„è®ºæ•° "nr_max_review":169
                total_match = re.search(r'"nr_max_review"\s*:\s*(\d+)', rev_resp.text)
                if total_match:
                    total_reviews = int(total_match.group(1))
                    # æ¯é¡µçº¦15æ¡ï¼Œè®¡ç®—æœ€åä¸€é¡µï¼ˆä½†ä¸è¶…è¿‡å®é™…é¡µæ•°ï¼‰
                    # å…ˆç”¨è¾ƒå°çš„ä¼°ç®—ï¼Œæ¯é¡µ20æ¡
                    max_page = max(1, (total_reviews + 19) // 20)
                    # è®¿é—®æœ€åä¸€é¡µ
                    last_page_url = f"{base_review_url}?p={max_page}"
                    print(f"      [DEBUG] è¯„è®ºæ€»æ•°: {total_reviews}, è®¿é—®æœ€åä¸€é¡µ: {last_page_url}")
                    rev_resp = requests.get(last_page_url, impersonate="chrome120", proxies=proxies, timeout=15)

                    # å¦‚æœæœ€åä¸€é¡µæ²¡æœ‰å†…å®¹ï¼Œé€é¡µå¾€å‰æ‰¾
                    decoded_text = rev_resp.text.replace('\\u002F', '/')
                    if '"orderDate"' not in decoded_text and max_page > 1:
                        for try_page in range(max_page - 1, 0, -1):
                            try_url = f"{base_review_url}?p={try_page}"
                            rev_resp = requests.get(try_url, impersonate="chrome120", proxies=proxies, timeout=15)
                            decoded_text = rev_resp.text.replace('\\u002F', '/')
                            if '"orderDate"' in decoded_text:
                                print(f"      [DEBUG] å®é™…æœ€åä¸€é¡µ: p={try_page}")
                                break
                else:
                    print(f"      [DEBUG] æ— æ³•è·å–è¯„è®ºæ€»æ•°ï¼Œä½¿ç”¨ç¬¬ä¸€é¡µ")

                # åŒ¹é…è¯„è®ºæ—¥æœŸ - ä»JSONä¸­æå– orderDate
                # æ ¼å¼: "orderDate":"2023\u002F04\u002F10" æˆ– "orderDate":"2023/04/10"
                # å…ˆè§£ç  \u002F ä¸º /
                decoded_text = rev_resp.text.replace('\\u002F', '/')

                # ä»JSONä¸­æå– orderDate
                dates = re.findall(r'"orderDate"\s*:\s*"(20\d{2})/(\d{1,2})/(\d{1,2})"', decoded_text)
                print(f"      [DEBUG] JSON orderDateåŒ¹é…: {dates[:5]}")

                # å¤‡ç”¨ï¼šä»é¡µé¢æ–‡æœ¬ä¸­åŒ¹é… æ³¨æ–‡æ—¥:2023/02/10
                if not dates:
                    dates = re.findall(r'æ³¨æ–‡æ—¥.{0,3}(20\d{2})/(\d{1,2})/(\d{1,2})', rev_resp.text)
                    print(f"      [DEBUG] æ³¨æ–‡æ—¥æ ¼å¼åŒ¹é…: {dates[:5]}")

                # è¿‡æ»¤æ— æ•ˆæ—¥æœŸï¼ˆæœˆä»½1-12ï¼Œæ—¥æœŸ1-31ï¼Œä¸”å¹´ä»½>=2010ï¼‰
                valid_dates = []
                for y, m, d in dates:
                    try:
                        year = int(y)
                        month = int(m)
                        day = int(d)
                        if 2010 <= year <= 2025 and 1 <= month <= 12 and 1 <= day <= 31:
                            valid_dates.append(f"{y}-{str(m).zfill(2)}-{str(d).zfill(2)}")
                    except:
                        pass

                print(f"      [DEBUG] æœ‰æ•ˆæ—¥æœŸ: {sorted(valid_dates)[:5]}")

                if valid_dates:
                    valid_dates.sort()
                    result["ä¸Šçº¿æ—¶é•¿"] = valid_dates[0]  # æœ€æ—©çš„è¯„è®ºæ—¥æœŸ
                else:
                    result["ä¸Šçº¿æ—¶é•¿"] = "æ—¥æœŸæå–å¤±è´¥"
            else:
                result["ä¸Šçº¿æ—¶é•¿"] = "æ— è¯„è®ºé“¾æ¥"

        return result

    except Exception as e:
        print(f"è¯¦æƒ…é¡µé”™è¯¯: {e}")
        return result


def extract_price(text):
    if not text: return "0"
    text = str(text).replace(',', '').replace(' ', '')
    match = re.search(r'(\d+)å††', text)
    if match: return match.group(1)
    match = re.search(r'[Â¥ï¿¥](\d+)', text)
    if match: return match.group(1)
    match = re.search(r'(\d{3,})', text)
    if match: return match.group(1)
    return "0"


def run_spider():
    print(f"ğŸš€ å¯åŠ¨ã€å®Œå…¨ä½“æ•´åˆç‰ˆã€‘çˆ¬è™« | å…³é”®è¯: {KEYWORD}")
    print(f"ğŸ“¡ ä»£ç†ç«¯å£: {PROXY_PORT} (è¯·ç¡®ä¿æ­£ç¡®)")
    print("-" * 60)

    base_url = "https://search.rakuten.co.jp/search/mall/{}/"
    raw_data = []

    # --- Step 1: åˆ—è¡¨æŠ“å– ---
    for page in range(1, PAGES_TO_SCRAPE + 1):
        url = base_url.format(KEYWORD) + f"?p={page}"
        print(f"ğŸ“¡ [Step 1] æŠ“å–ç¬¬ {page} é¡µåˆ—è¡¨...")

        try:
            res = requests.get(url, impersonate="chrome120", timeout=30, proxies=proxies)
            soup = BeautifulSoup(res.text, 'html.parser')

            # å¸‚åœºé¥±å’Œåº¦ - æå–æ€»å•†å“æ•°ï¼ˆæ ¼å¼ï¼š394,408ä»¶ï¼‰
            total_txt = "æœªçŸ¥"
            page_text = soup.get_text()

            # æ–¹æ³•1: åŒ¹é… "æ¤œç´¢çµæœ1ï½45ä»¶ï¼ˆ394,408ä»¶ï¼‰" æ ¼å¼
            count_match = re.search(r'[\(ï¼ˆ]([\d,]+)ä»¶[\)ï¼‰]', page_text)
            if count_match and len(count_match.group(1).replace(',', '')) >= 3:
                total_txt = count_match.group(1)

            # æ–¹æ³•2: åŒ¹é… "394,408ä»¶ä¸­" æˆ– "394408 ä»¶"
            if total_txt == "æœªçŸ¥":
                count_match = re.search(r'([\d,]{5,})ä»¶', page_text)
                if count_match:
                    total_txt = count_match.group(1)

            # æ–¹æ³•3: ä» span.count ç­‰å…ƒç´ æå–
            if total_txt == "æœªçŸ¥":
                count_el = soup.select_one('.search-count') or soup.select_one('[class*="count"]')
                if count_el:
                    m = re.search(r'([\d,]+)', count_el.get_text())
                    if m and len(m.group(1).replace(',', '')) >= 3:
                        total_txt = m.group(1)

            print(f"   ğŸ“Š æœç´¢ç»“æœæ€»æ•°: {total_txt}ä»¶")

            items = soup.select('.searchresultitem')
            if not items: items = soup.select('div[data-track-item]')

            # DEBUG: ä¿å­˜HTMLç”¨äºåˆ†æ
            with open("debug_list_page.html", "w", encoding="utf-8") as f:
                f.write(res.text)
            print(f"      [DEBUG] å·²ä¿å­˜åˆ—è¡¨é¡µHTMLåˆ° debug_list_page.html")
            print(f"      [DEBUG] æ‰¾åˆ° {len(items)} ä¸ªå•†å“å…ƒç´  (é€‰æ‹©å™¨: .searchresultitem)")

            # æ‰“å°ç¬¬ä¸€ä¸ªitemçš„æ‰€æœ‰aæ ‡ç­¾
            if items:
                first_item = items[0]
                all_links = first_item.select('a[href]')
                print(f"      [DEBUG] ç¬¬1ä¸ªå•†å“å†…æœ‰ {len(all_links)} ä¸ªé“¾æ¥")
                for i, a in enumerate(all_links[:5]):
                    print(f"        é“¾æ¥{i + 1}: {a.get('href', '')[:80]}")

            # é™åˆ¶åˆ—è¡¨é¡µæŠ“å–æ•°é‡
            if LIST_SCRAPE_LIMIT:
                items = items[:LIST_SCRAPE_LIMIT]

            ad_rank, nat_rank = 0, 0

            for idx, item in enumerate(items):
                try:
                    full_text = " ".join(item.get_text().split())

                    # å¤šç§æ–¹å¼å°è¯•è·å–å•†å“é“¾æ¥
                    link = "N/A"
                    title = "N/A"
                    review_link = ""  # ä¿å­˜è¯„è®ºé“¾æ¥ä½œä¸ºå¤‡ç”¨

                    # ğŸ”¥ æ–¹å¼1: ä¿å­˜è¯„è®ºé“¾æ¥ï¼ˆç”¨äºå¤‡ç”¨è§£æï¼‰
                    for a in item.select('a[href*="review.rakuten.co.jp"]'):
                        href = a.get('href', '')
                        if 'review.rakuten.co.jp/item/' in href:
                            review_link = href
                            break

                    # æ–¹å¼2: ç›´æ¥æ‰¾åŒ…å« item.rakuten.co.jp çš„é“¾æ¥
                    for a in item.select('a[href*="item.rakuten.co.jp"]'):
                        href = a.get('href', '')
                        if 'item.rakuten.co.jp' in href:
                            link = href
                            title = a.get_text(strip=True) or "N/A"
                            break

                    # æ–¹å¼3: .title a æˆ– h2 aï¼ˆå¯èƒ½æ˜¯redirecté“¾æ¥ï¼‰
                    if link == "N/A":
                        title_tag = item.select_one('.title a') or item.select_one('h2 a')
                        if title_tag:
                            link = title_tag.get('href', 'N/A')
                            title = title_tag.get_text(strip=True) or "N/A"

                    # æ–¹å¼4: ä» data-track-ratid æˆ– data-item-id å±æ€§è·å–
                    if link == "N/A" or 'redirect' in link:
                        # æŸ¥æ‰¾å•†å“IDå±æ€§
                        item_id_attr = item.get('data-track-ratid') or item.get('data-item-id') or ""
                        if item_id_attr:
                            # æ ¼å¼å¯èƒ½æ˜¯ "åº—é“ºå:å•†å“ID"
                            parts = item_id_attr.split(':')
                            if len(parts) >= 2:
                                shop_name = parts[0]
                                product_id = parts[1]
                                link = f"https://item.rakuten.co.jp/{shop_name}/{product_id}/"
                                print(f"      [DEBUG] ä»å±æ€§æ„å»ºé“¾æ¥: {link}")

                    # ğŸ”¥ æ–¹å¼5: å¦‚æœlinkä»æ˜¯redirectï¼Œé™„åŠ è¯„è®ºé“¾æ¥ç”¨äºå¤‡ç”¨è§£æ
                    if review_link and ('redirect' in link or 'grp' in link):
                        # ç”¨ç‰¹æ®Šæ ¼å¼ä¿å­˜ï¼šåŸå§‹é“¾æ¥|||è¯„è®ºé“¾æ¥
                        link = f"{link}|||{review_link}"

                    # è°ƒè¯•ï¼šæ˜¾ç¤ºåŸå§‹é“¾æ¥ï¼ˆæœ«å°¾éƒ¨åˆ†ï¼ŒåŒºåˆ†ä¸åŒé“¾æ¥ï¼‰
                    link_suffix = link[-50:] if len(link) > 50 else link
                    print(f"      [åˆ—è¡¨DEBUG] #{idx + 1} é“¾æ¥æœ«å°¾: ...{link_suffix}")
                    shop = item.select_one('.merchant a').get_text(strip=True) if item.select_one(
                        '.merchant a') else "N/A"
                    # ä¸»å›¾è·å– - å¤šç§é€‰æ‹©å™¨
                    img = "N/A"
                    # ä¼˜å…ˆè·å–å•†å“å›¾ç‰‡ï¼ˆæ’é™¤å›¾æ ‡ï¼‰
                    for img_el in item.select('img'):
                        img_url = img_el.get('src') or img_el.get('data-src') or img_el.get('data-lazy') or ""
                        # è¿‡æ»¤æ‰éå•†å“å›¾ç‰‡
                        if not img_url:
                            continue
                        # æ’é™¤: svgå›¾æ ‡ã€assetsèµ„æºã€logoã€iconç­‰
                        if any(x in img_url.lower() for x in
                               ['.svg', '/assets/', '/resources/', 'logo', 'icon', 'badge', '39shop']):
                            continue
                        # å¿…é¡»æ˜¯å›¾ç‰‡æ ¼å¼
                        if any(x in img_url.lower() for x in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                            img = img_url
                            break
                    # æ¸…ç†URL
                    if img and img != "N/A":
                        img = re.sub(r'\?.*$', '', img)  # å»æ‰URLå‚æ•°
                        img = img.replace('_ex=80x80', '').replace('_ex=128x128', '')  # å»æ‰å°ºå¯¸é™åˆ¶

                    # ä»·æ ¼æå–
                    price = extract_price(full_text)
                    if price == "0":
                        pt = item.select_one('[class*="price"]')
                        if pt: price = extract_price(pt.get_text())

                    # Review - æ›´ç²¾ç¡®çš„æå–
                    rev_cnt, rev_score = 0, "0.0"

                    # è¯„åˆ†ï¼šåŒ¹é… â˜…4.67 æˆ– è©•ä¾¡4.67 æˆ– (4.67) æ ¼å¼
                    score_match = re.search(r'[â˜…â˜†è©•ä¾¡]\s*(\d\.\d{1,2})', full_text)
                    if not score_match:
                        # å¤‡é€‰ï¼šåŒ¹é…è¯„è®ºæ•°å‰çš„å°æ•°ï¼Œå¦‚ "4.67(169ä»¶)"
                        score_match = re.search(r'(\d\.\d{1,2})\s*[\(ï¼ˆ]\d+[ä»¶\)ï¼‰]', full_text)
                    if score_match:
                        score_val = float(score_match.group(1))
                        if 1.0 <= score_val <= 5.0:
                            rev_score = score_match.group(1)

                    # è¯„è®ºæ•°ï¼šåŒ¹é… (169ä»¶) æˆ– ï¼ˆ169ä»¶ï¼‰
                    cnt_match = re.search(r'[\(ï¼ˆ](\d{1,6})[ä»¶]?[\)ï¼‰]', full_text)
                    if cnt_match:
                        rev_cnt = int(cnt_match.group(1))

                    # æ’å
                    is_ad = "[PR]" in title or item.select_one('.marker-pr') or "r.rakuten.co.jp" in link
                    if is_ad:
                        ad_rank += 1
                    else:
                        nat_rank += 1

                    sat = f"å•†å“æ•°{total_txt}ä»¶ä¸­, {('PR' if is_ad else 'è‡ªç„¶')}{ad_rank if is_ad else nat_rank}ä½"

                    # è¿™é‡Œå…ˆåˆå§‹åŒ–æ‰€æœ‰çš„keyï¼Œé˜²æ­¢åé¢æŠ¥é”™
                    raw_data.append({
                        "å“ç‰Œ": shop, "æ ‡é¢˜": title, "url": link, "ä¸»å›¾": img, "é¥±å’Œåº¦": sat,
                        "è¯„è®ºæ•°": rev_cnt, "è¯„åˆ†": rev_score, "ä»·æ ¼": int(price),
                        "ä¸Šçº¿æ—¶é•¿": "...", "é¢„ä¼°æœˆé”€": "", "å•†å“è¯¦ç»†å‚æ•°": "...",
                        "æ ¸å¿ƒå–ç‚¹åˆ†æ": "...", "è¯„è®ºå‡ºç°ä¼˜ç‚¹": "...", "å®¢è¯‰ç‚¹": "...",
                        "æœ‰æ— è§†é¢‘": "...", "å¤§ç±»æ’å": "", "å°ç±»æ’å": "", "å¤‡æ³¨": "",
                        "ç‰¹å¾1": "", "ç‰¹å¾2": "", "ç‰¹å¾3": "", "ç‰¹å¾4": "", "ç‰¹å¾5": "",
                        "ç‰¹å¾6": "", "ç‰¹å¾7": "", "ç‰¹å¾8": "", "ç‰¹å¾9": ""
                    })
                except:
                    continue
            print(f"   âœ… æœ¬é¡µè·å– {len(items)} æ¡æ•°æ®")
            time.sleep(2)
        except Exception as e:
            print(f"é”™è¯¯: {e}")

    # --- Step 2: æ·±åº¦æŠ“å– ---
    df = pd.DataFrame(raw_data)
    if df.empty: return
    limit = len(df) if DEEP_SCRAPE_LIMIT is None else min(len(df), DEEP_SCRAPE_LIMIT)

    print("-" * 60)
    print(f"ğŸ•µï¸ [Step 2] æ·±åº¦æŠ“å–å‚æ•°ã€æ—¶é—´ã€è¯„è®ºåˆ†æ (å…± {limit} æ¡)...")

    for i in range(limit):
        row = df.iloc[i]
        is_pr = "PRå¹¿å‘Š" if "grp" in str(row['url']) else "æ™®é€š"
        print(f"   [{i + 1}/{limit}] åˆ†æä¸­... [{is_pr}]")
        print(f"      [DEBUG] åŸå§‹URL: {str(row['url'])[:80]}...")

        # è°ƒç”¨æ ¸å¿ƒæ·±åº¦æŠ“å–å‡½æ•° (åŒæ—¶è·å–æ—¶é—´+å‚æ•°+å–ç‚¹+è¯„è®º)
        details = get_product_details(row['url'], row['è¯„è®ºæ•°'])

        # æ£€æŸ¥æ˜¯å¦è§£ææˆåŠŸ
        if details['ä¸Šçº¿æ—¶é•¿'] == 'URLè§£æå¤±è´¥':
            print(f"      âš ï¸ URLè§£æå¤±è´¥ï¼Œè·³è¿‡æ­¤å•†å“")

        df.at[i, 'ä¸Šçº¿æ—¶é•¿'] = details['ä¸Šçº¿æ—¶é•¿']
        df.at[i, 'å•†å“è¯¦ç»†å‚æ•°'] = details['å•†å“è¯¦ç»†å‚æ•°']
        # âœ¨ å¡«å…¥æ–°æ•°æ®
        df.at[i, 'æ ¸å¿ƒå–ç‚¹åˆ†æ'] = details['æ ¸å¿ƒå–ç‚¹åˆ†æ']
        df.at[i, 'è¯„è®ºå‡ºç°ä¼˜ç‚¹'] = details['è¯„è®ºå‡ºç°ä¼˜ç‚¹']
        df.at[i, 'å®¢è¯‰ç‚¹'] = details['å®¢è¯‰ç‚¹']
        df.at[i, 'æœ‰æ— è§†é¢‘'] = details['æœ‰æ— è§†é¢‘']
        df.at[i, 'å¤§ç±»æ’å'] = details['å¤§ç±»æ’å']
        df.at[i, 'å°ç±»æ’å'] = details['å°ç±»æ’å']
        df.at[i, 'å¤‡æ³¨'] = details['å¤‡æ³¨']

        # âœ¨ ä¿å­˜AIæå–çš„äº§å“ç‰¹å¾
        if details.get('äº§å“ç‰¹å¾'):
            for j, feat in enumerate(details['äº§å“ç‰¹å¾'][:9], 1):
                df.at[i, f'ç‰¹å¾{j}'] = feat

        # å¦‚æœè¯¦æƒ…é¡µè·å–åˆ°äº†æ›´å¥½çš„ä¸»å›¾ï¼Œæ›´æ–°å®ƒ
        if details.get('ä¸»å›¾') and (df.at[i, 'ä¸»å›¾'] == "N/A" or not df.at[i, 'ä¸»å›¾']):
            df.at[i, 'ä¸»å›¾'] = details['ä¸»å›¾']

        # âœ¨ ç”¨è¯¦æƒ…é¡µæ•°æ®è¡¥å……ç¼ºå¤±çš„è¯„è®ºæ•°å’Œè¯„åˆ†
        if details.get('è¯„è®ºæ•°_è¡¥å……') is not None:
            current_cnt = df.at[i, 'è¯„è®ºæ•°']
            # æ£€æŸ¥æ˜¯å¦ä¸º0æˆ–ç¼ºå¤±
            try:
                if int(current_cnt) == 0:
                    df.at[i, 'è¯„è®ºæ•°'] = details['è¯„è®ºæ•°_è¡¥å……']
                    print(f"      â†’ è¡¥å……è¯„è®ºæ•°: {details['è¯„è®ºæ•°_è¡¥å……']}")
            except:
                df.at[i, 'è¯„è®ºæ•°'] = details['è¯„è®ºæ•°_è¡¥å……']
                print(f"      â†’ è¡¥å……è¯„è®ºæ•°: {details['è¯„è®ºæ•°_è¡¥å……']}")
        if details.get('è¯„åˆ†_è¡¥å……') is not None:
            current_score = df.at[i, 'è¯„åˆ†']
            # æ£€æŸ¥æ˜¯å¦ä¸º0æˆ–"0.0"æˆ–ç¼ºå¤±
            try:
                if float(current_score) == 0.0:
                    df.at[i, 'è¯„åˆ†'] = details['è¯„åˆ†_è¡¥å……']
                    print(f"      â†’ è¡¥å……è¯„åˆ†: {details['è¯„åˆ†_è¡¥å……']}")
            except:
                df.at[i, 'è¯„åˆ†'] = details['è¯„åˆ†_è¡¥å……']
                print(f"      â†’ è¡¥å……è¯„åˆ†: {details['è¯„åˆ†_è¡¥å……']}")

        # try:
        #     df.at[i, 'é¢„ä¼°æœˆé”€'] = f"Reviews score: {int(df.at[i, 'è¯„è®ºæ•°'] * 1.5)}"
        # except:
        #     pass

        time.sleep(random.uniform(1.2, 2.5))

    print("\nâœ… æŠ“å–å®Œæˆ")

    # --- Step 3: æ•´ç†æ•°æ®å¹¶ä¿å­˜ ---
    print("-" * 60)
    print("ğŸ“Š [Step 3] æ•´ç†æ•°æ®æ ¼å¼...")

    # å¤åˆ¶dfç”¨äºå¤„ç†
    df_jp = df.copy()

    # æœ€ç»ˆåˆ—é¡ºåº - åŒ¹é…ç›®æ ‡è¡¨æ ¼æ ¼å¼
    # åŸºæœ¬ä¿¡æ¯
    df_jp['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'] = KEYWORD
    df_jp['ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼'] = df_jp['æ ‡é¢˜']  # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºCatch Copy
    df_jp['å“ç‰Œï¼ˆåº—é“ºåï¼‰'] = df_jp['å“ç‰Œ']

    # å”®å–ä¿¡æ¯ - æ·»åŠ ç¼ºå¤±åˆ—
    df_jp['å¸‚åœºé¥±å’Œåº¦'] = df_jp['é¥±å’Œåº¦']
    df_jp['å°ç±»æ’å'] = df['å°ç±»æ’å']
    df_jp['å¤§ç±»æ’å'] = df['å¤§ç±»æ’å']
    df_jp['reviewæ•°é‡'] = df_jp['è¯„è®ºæ•°']
    df_jp['reviewè¯„åˆ†'] = df_jp['è¯„åˆ†']
    df_jp['ä»·æ ¼ï¼ˆJPY)'] = df_jp['ä»·æ ¼']
    df_jp['ä¸Šçº¿æ—¶é•¿ï¼ˆæœˆï¼‰'] = df_jp['ä¸Šçº¿æ—¶é•¿']
    df_jp['æœˆé”€å”®é¢'] = ""  # éœ€è¦è®¡ç®—

    # äº§å“ç‰¹å¾ - ä½¿ç”¨AIæå–çš„ç‰¹å¾ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨å‚æ•°å¡«å……
    for i in range(1, 10):
        if f'ç‰¹å¾{i}' not in df_jp.columns:
            df_jp[f'ç‰¹å¾{i}'] = ""

    # å¤‡é€‰ï¼šå¦‚æœAIç‰¹å¾ä¸ºç©ºï¼Œç”¨å•†å“è¯¦ç»†å‚æ•°å¡«å……
    for idx, row in df_jp.iterrows():
        # æ£€æŸ¥æ˜¯å¦æœ‰AIæå–çš„ç‰¹å¾
        has_ai_features = any(str(row.get(f'ç‰¹å¾{i}', '')).strip() for i in range(1, 10))
        if not has_ai_features:
            params = str(row.get('å•†å“è¯¦ç»†å‚æ•°', '')).split('\n')
            for i, param in enumerate(params[:9], 1):
                df_jp.at[idx, f'ç‰¹å¾{i}'] = param

    # ç­–ç•¥åˆ†æä¸å¤ç›˜ - ç©ºåˆ—ä¾›æ‰‹åŠ¨å¡«å†™
    df_jp['é¢„ä¼°å”®ä»·'] = ""
    df_jp['ä¾›åº”å•†æ˜¯å¦å¯ä»¥å¼€å‘ç¥¨ï¼Œé‡‡è´­å¤šå°‘ä¸ªå¯ä»¥å¼€'] = ""
    df_jp['å¹³å‡æ¯›åˆ©ç‡'] = ""
    df_jp['ä¿ƒé”€é¢‘ç‡'] = ""
    df_jp['å¯ä¼˜åŒ–æ–¹å‘'] = ""
    df_jp['ä¼˜å…ˆçº§'] = ""

    # æœ€ç»ˆåˆ—é¡ºåº
    final_cols = [
        # åŸºæœ¬ä¿¡æ¯
        "å“ç‰Œï¼ˆåº—é“ºåï¼‰", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼", "url", "ä¸»å›¾",
        # å”®å–ä¿¡æ¯
        "å¸‚åœºé¥±å’Œåº¦", "å°ç±»æ’å", "å¤§ç±»æ’å", "reviewæ•°é‡", "reviewè¯„åˆ†",
        "ä»·æ ¼ï¼ˆJPY)", "ä¸Šçº¿æ—¶é•¿ï¼ˆæœˆï¼‰", "é¢„ä¼°æœˆé”€", "æœˆé”€å”®é¢",
        # äº§å“ç‰¹å¾ä¸ç”¨æˆ·ä½“éªŒ
        "ç‰¹å¾1", "ç‰¹å¾2", "ç‰¹å¾3", "ç‰¹å¾4", "ç‰¹å¾5", "ç‰¹å¾6", "ç‰¹å¾7", "ç‰¹å¾8", "ç‰¹å¾9",
        "æ ¸å¿ƒå–ç‚¹åˆ†æ", "è¯„è®ºå‡ºç°ä¼˜ç‚¹", "å®¢è¯‰ç‚¹", "æœ‰æ— è§†é¢‘", "å¤‡æ³¨",
        # ç­–ç•¥åˆ†æä¸å¤ç›˜
        "é¢„ä¼°å”®ä»·", "ä¾›åº”å•†æ˜¯å¦å¯ä»¥å¼€å‘ç¥¨ï¼Œé‡‡è´­å¤šå°‘ä¸ªå¯ä»¥å¼€", "å¹³å‡æ¯›åˆ©ç‡", "ä¿ƒé”€é¢‘ç‡", "å¯ä¼˜åŒ–æ–¹å‘", "ä¼˜å…ˆçº§"
    ]

    # ç¡®ä¿åˆ—éƒ½å­˜åœ¨
    df_jp = df_jp.reindex(columns=final_cols)

    # ä¸­æ–‡ç‰ˆ
    df_cn = df_jp.copy()
    if ENABLE_TRANSLATION:
        print("ğŸ‡¨ğŸ‡³ æ­£åœ¨ç¿»è¯‘ä¸­æ–‡ç‰ˆ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
        cols_to_translate = ["å“ç‰Œï¼ˆåº—é“ºåï¼‰", "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼", "å¸‚åœºé¥±å’Œåº¦",
                             "æ ¸å¿ƒå–ç‚¹åˆ†æ", "è¯„è®ºå‡ºç°ä¼˜ç‚¹", "å®¢è¯‰ç‚¹",
                             "ç‰¹å¾1", "ç‰¹å¾2", "ç‰¹å¾3", "ç‰¹å¾4", "ç‰¹å¾5",
                             "ç‰¹å¾6", "ç‰¹å¾7", "ç‰¹å¾8", "ç‰¹å¾9"]
        total_cols = len(cols_to_translate)
        for idx, col in enumerate(cols_to_translate, 1):
            if col in df_cn.columns:
                print(f"   [{idx}/{total_cols}] æ­£åœ¨ç¿»è¯‘: {col} ...")
                try:
                    df_cn[col] = df_cn[col].apply(safe_translate)
                except Exception as e:
                    print(f"   âš ï¸ ç¿»è¯‘ {col} å¤±è´¥: {e}")

        # ç¿»è¯‘æ’åä¸­çš„ç±»ç›®å
        def translate_rank(rank_text):
            if not rank_text or not isinstance(rank_text, str):
                return rank_text
            match = re.match(r'(.+?)\s*ç¬¬(\d+)', rank_text)
            if match:
                cat_name_jp = match.group(1)
                rank_num = match.group(2)
                cat_name_cn = safe_translate(cat_name_jp)
                return f"{cat_name_cn} ç¬¬{rank_num}"
            return rank_text

        print("   [ç¿»è¯‘æ’å...]")
        df_cn['å¤§ç±»æ’å'] = df_cn['å¤§ç±»æ’å'].apply(translate_rank)
        df_cn['å°ç±»æ’å'] = df_cn['å°ç±»æ’å'].apply(translate_rank)

        print("   âœ… ç¿»è¯‘å®Œæˆ")
    else:
        print("â­ï¸ è·³è¿‡ç¿»è¯‘ (ENABLE_TRANSLATION=False)")

    def save_excel_with_format(df, filename, add_group_headers=True):
        """ä¿å­˜Excelå¹¶è®¾ç½®æ ¼å¼ï¼šåˆ†ç»„æ ‡é¢˜ã€è‡ªåŠ¨æ¢è¡Œã€è°ƒæ•´åˆ—å®½ã€è°ƒæ•´è¡Œé«˜"""
        try:
            from openpyxl import Workbook
            from openpyxl.styles import Alignment, Font, PatternFill, Border, Side
            from openpyxl.utils import get_column_letter

            wb = Workbook()
            ws = wb.active

            # å®šä¹‰åˆ†ç»„ä¿¡æ¯
            groups = [
                ("åŸºæœ¬ä¿¡æ¯", 5),  # 5åˆ—
                ("å”®å–ä¿¡æ¯", 9),  # 9åˆ—
                ("äº§å“ç‰¹å¾ä¸ç”¨æˆ·ä½“éªŒ", 14),  # 14åˆ—
                ("ç­–ç•¥åˆ†æä¸å¤ç›˜", 6)  # 6åˆ—
            ]

            # æ ·å¼å®šä¹‰
            header_fill = PatternFill(start_color="FFF2CC", end_color="FFF2CC", fill_type="solid")
            group_fill = PatternFill(start_color="D9E1F2", end_color="D9E1F2", fill_type="solid")
            thin_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )

            start_row = 1
            if add_group_headers:
                # å†™å…¥åˆ†ç»„æ ‡é¢˜è¡Œ (ç¬¬1è¡Œ)
                col_idx = 1
                for group_name, group_cols in groups:
                    cell = ws.cell(row=1, column=col_idx, value=group_name)
                    cell.font = Font(bold=True)
                    cell.fill = group_fill
                    cell.alignment = Alignment(horizontal='center', vertical='center')
                    cell.border = thin_border
                    # åˆå¹¶å•å…ƒæ ¼
                    if group_cols > 1:
                        ws.merge_cells(start_row=1, start_column=col_idx, end_row=1,
                                       end_column=col_idx + group_cols - 1)
                    col_idx += group_cols
                start_row = 2

            # å†™å…¥åˆ—å¤´ (ç¬¬2è¡Œ)
            for c_idx, col_name in enumerate(df.columns, 1):
                cell = ws.cell(row=start_row, column=c_idx, value=col_name)
                cell.font = Font(bold=True)
                cell.fill = header_fill
                cell.alignment = Alignment(wrap_text=True, vertical='center', horizontal='center')
                cell.border = thin_border

            # æ‰¾åˆ°ä¸»å›¾åˆ—çš„ç´¢å¼•
            img_col_idx = None
            for idx, col_name in enumerate(df.columns, 1):
                if col_name == "ä¸»å›¾":
                    img_col_idx = idx
                    break

            # å†™å…¥æ•°æ® (ä»ç¬¬3è¡Œå¼€å§‹)
            for r_idx, row in enumerate(df.values, start_row + 1):
                for c_idx, value in enumerate(row, 1):
                    cell = ws.cell(row=r_idx, column=c_idx, value=value)
                    cell.alignment = Alignment(wrap_text=True, vertical='top')
                    cell.border = thin_border

                    # å¦‚æœæ˜¯ä¸»å›¾åˆ—ä¸”å¯ç”¨äº†å›¾ç‰‡ä¸‹è½½ï¼ŒåµŒå…¥å›¾ç‰‡
                    if DOWNLOAD_IMAGES and c_idx == img_col_idx and value and value != "N/A":
                        try:
                            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                            img_filename = f"{IMAGE_FOLDER}/img_{r_idx - start_row}.jpg"
                            local_path = download_image(value, img_filename)
                            if local_path and os.path.exists(local_path):
                                # åµŒå…¥å›¾ç‰‡åˆ°å•å…ƒæ ¼
                                img = XLImage(local_path)
                                img.width = 60
                                img.height = 60
                                # å®šä½åˆ°å•å…ƒæ ¼
                                col_letter = get_column_letter(c_idx)
                                img.anchor = f"{col_letter}{r_idx}"
                                ws.add_image(img)
                                # æ¸…ç©ºå•å…ƒæ ¼æ–‡å­—ï¼ˆåªä¿ç•™å›¾ç‰‡ï¼‰
                                cell.value = ""
                        except Exception as e:
                            pass  # å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œä¿ç•™URL

            # è®¾ç½®åˆ—å®½ - ä½¿ç”¨ get_column_letter å‡½æ•°
            col_widths = {
                "å“ç‰Œï¼ˆåº—é“ºåï¼‰": 18, "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": 15, "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼": 35, "url": 20, "ä¸»å›¾": 15,
                "å¸‚åœºé¥±å’Œåº¦": 25, "å°ç±»æ’å": 12, "å¤§ç±»æ’å": 12, "reviewæ•°é‡": 10, "reviewè¯„åˆ†": 10,
                "ä»·æ ¼ï¼ˆJPY)": 12, "ä¸Šçº¿æ—¶é•¿ï¼ˆæœˆï¼‰": 14, "é¢„ä¼°æœˆé”€": 12, "æœˆé”€å”®é¢": 12,
                "ç‰¹å¾1": 20, "ç‰¹å¾2": 20, "ç‰¹å¾3": 20, "ç‰¹å¾4": 20, "ç‰¹å¾5": 20,
                "ç‰¹å¾6": 20, "ç‰¹å¾7": 20, "ç‰¹å¾8": 20, "ç‰¹å¾9": 20,
                "æ ¸å¿ƒå–ç‚¹åˆ†æ": 35, "è¯„è®ºå‡ºç°ä¼˜ç‚¹": 40, "å®¢è¯‰ç‚¹": 40, "æœ‰æ— è§†é¢‘": 10, "å¤‡æ³¨": 15,
                "é¢„ä¼°å”®ä»·": 12, "ä¾›åº”å•†æ˜¯å¦å¯ä»¥å¼€å‘ç¥¨ï¼Œé‡‡è´­å¤šå°‘ä¸ªå¯ä»¥å¼€": 20,
                "å¹³å‡æ¯›åˆ©ç‡": 12, "ä¿ƒé”€é¢‘ç‡": 12, "å¯ä¼˜åŒ–æ–¹å‘": 20, "ä¼˜å…ˆçº§": 10
            }

            for col_idx, col_name in enumerate(df.columns, 1):
                col_letter = get_column_letter(col_idx)
                ws.column_dimensions[col_letter].width = col_widths.get(col_name, 15)

            # è®¾ç½®è¡Œé«˜
            ws.row_dimensions[1].height = 25  # åˆ†ç»„æ ‡é¢˜è¡Œ
            ws.row_dimensions[2].height = 30  # åˆ—å¤´è¡Œ
            for row_idx in range(3, ws.max_row + 1):
                ws.row_dimensions[row_idx].height = 80

            # å†»ç»“å‰ä¸¤è¡Œ
            ws.freeze_panes = 'A3'

            wb.save(filename)
            print(f"ğŸ‰ å·²ä¿å­˜: {filename}")
        except Exception as e:
            print(f"Excelä¿å­˜å¤±è´¥({e})ï¼Œå°è¯•CSV...")
            df.to_csv(filename.replace('.xlsx', '.csv'), index=False, encoding='utf-8-sig')

    if DOWNLOAD_IMAGES:
        print(f"ğŸ–¼ï¸ å¯ç”¨å›¾ç‰‡ä¸‹è½½ï¼Œå›¾ç‰‡å°†ä¿å­˜åˆ° {IMAGE_FOLDER}/ æ–‡ä»¶å¤¹å¹¶åµŒå…¥Excel")

    save_excel_with_format(df_jp, "rakuten_complete_JP.xlsx")
    save_excel_with_format(df_cn, "rakuten_complete_CN.xlsx")


if __name__ == "__main__":
    run_spider()