"""
ä¹å¤©å•†å“çˆ¬è™« Web åº”ç”¨ - å®Œæ•´ç‰ˆ
ç›´æ¥ä½¿ç”¨åŸå§‹çˆ¬è™«ä»£ç çš„å…¨éƒ¨åŠŸèƒ½
"""
import os
import io
import re
import sys
import time
import random
import threading
import uuid
from datetime import datetime
from flask import Flask, render_template, request, jsonify, send_file, Response
from flask_cors import CORS
import pandas as pd

# å¯¼å…¥åŸå§‹çˆ¬è™«æ¨¡å—çš„æ‰€æœ‰å‡½æ•°
from scraper import (
    proxies, PROXY_PORT,
    safe_translate,
    extract_price,
    resolve_pr_link,
    extract_dynamic_specs,
    check_has_video,
    extract_selling_points,
    analyze_selling_points_ai,
    extract_product_features,
    get_categories_from_page,
    get_category_ranking,
    extract_ranking_info,
    analyze_reviews,
    get_product_details,
    USE_AI_FEATURES,
    ENABLE_TRANSLATION,
)
from curl_cffi import requests as cffi_requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
from openpyxl.styles import Alignment, Font, PatternFill, Border, Side
from openpyxl.utils import get_column_letter

app = Flask(__name__)
CORS(app)

# ä»»åŠ¡å­˜å‚¨
tasks = {}

# å®æ—¶æ—¥å¿—å­˜å‚¨
task_logs = {}


def log_message(task_id, message):
    """è®°å½•ä»»åŠ¡æ—¥å¿—"""
    if task_id not in task_logs:
        task_logs[task_id] = []
    timestamp = datetime.now().strftime("%H:%M:%S")
    task_logs[task_id].append(f"[{timestamp}] {message}")
    print(f"[{task_id[:8]}] {message}")


def run_scraper_full(task_id, keyword, pages=1, deep_limit=5, enable_ai=True, enable_translate=True):
    """æ‰§è¡Œå®Œæ•´ç‰ˆçˆ¬è™«ä»»åŠ¡"""
    task = tasks[task_id]
    task['status'] = 'running'
    task['progress'] = 0
    task['message'] = f'å¼€å§‹æœç´¢å…³é”®è¯: {keyword}'
    log_message(task_id, f"ğŸš€ å¯åŠ¨çˆ¬è™« | å…³é”®è¯: {keyword} | é¡µæ•°: {pages} | æ·±åº¦: {deep_limit}")

    base_url = "https://search.rakuten.co.jp/search/mall/{}/"
    raw_data = []

    try:
        # ==================== Step 1: åˆ—è¡¨æŠ“å– ====================
        for page in range(1, pages + 1):
            url = base_url.format(keyword) + f"?p={page}"
            task['message'] = f'æ­£åœ¨æŠ“å–ç¬¬ {page}/{pages} é¡µåˆ—è¡¨...'
            log_message(task_id, f"ğŸ“¡ æŠ“å–åˆ—è¡¨é¡µ {page}/{pages}: {url}")

            res = cffi_requests.get(url, impersonate="chrome120", timeout=30, proxies=proxies)
            soup = BeautifulSoup(res.text, 'html.parser')

            # å¸‚åœºé¥±å’Œåº¦
            page_text = soup.get_text()
            total_txt = "æœªçŸ¥"
            count_match = re.search(r'[\(ï¼ˆ]([\d,]+)ä»¶[\)ï¼‰]', page_text)
            if count_match and len(count_match.group(1).replace(',', '')) >= 3:
                total_txt = count_match.group(1)

            if total_txt == "æœªçŸ¥":
                count_match = re.search(r'([\d,]{5,})ä»¶', page_text)
                if count_match:
                    total_txt = count_match.group(1)

            log_message(task_id, f"   ğŸ“Š æœç´¢ç»“æœæ€»æ•°: {total_txt}ä»¶")

            items = soup.select('.searchresultitem')
            if not items:
                items = soup.select('div[data-track-item]')

            log_message(task_id, f"   æ‰¾åˆ° {len(items)} ä¸ªå•†å“å…ƒç´ ")

            ad_rank, nat_rank = 0, 0

            for idx, item in enumerate(items):
                try:
                    full_text = " ".join(item.get_text().split())

                    link = "N/A"
                    title = "N/A"
                    review_link = ""

                    # è¯„è®ºé“¾æ¥
                    for a in item.select('a[href*="review.rakuten.co.jp"]'):
                        href = a.get('href', '')
                        if 'review.rakuten.co.jp/item/' in href:
                            review_link = href
                            break

                    # å•†å“é“¾æ¥
                    for a in item.select('a[href*="item.rakuten.co.jp"]'):
                        href = a.get('href', '')
                        if 'item.rakuten.co.jp' in href:
                            link = href
                            title = a.get_text(strip=True) or "N/A"
                            break

                    if link == "N/A":
                        title_tag = item.select_one('.title a') or item.select_one('h2 a')
                        if title_tag:
                            link = title_tag.get('href', 'N/A')
                            title = title_tag.get_text(strip=True) or "N/A"

                    # ä»å±æ€§è·å–é“¾æ¥
                    if link == "N/A" or 'redirect' in link:
                        item_id_attr = item.get('data-track-ratid') or item.get('data-item-id') or ""
                        if item_id_attr:
                            parts = item_id_attr.split(':')
                            if len(parts) >= 2:
                                shop_name = parts[0]
                                product_id = parts[1]
                                link = f"https://item.rakuten.co.jp/{shop_name}/{product_id}/"

                    if review_link and ('redirect' in link or 'grp' in link):
                        link = f"{link}|||{review_link}"

                    shop = item.select_one('.merchant a').get_text(strip=True) if item.select_one('.merchant a') else "N/A"

                    # ä¸»å›¾
                    img = "N/A"
                    for img_el in item.select('img'):
                        img_url = img_el.get('src') or img_el.get('data-src') or img_el.get('data-lazy') or ""
                        if not img_url:
                            continue
                        if any(x in img_url.lower() for x in ['.svg', '/assets/', '/resources/', 'logo', 'icon', 'badge', '39shop']):
                            continue
                        if any(x in img_url.lower() for x in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                            img = img_url
                            break
                    if img and img != "N/A":
                        img = re.sub(r'\?.*$', '', img)
                        img = img.replace('_ex=80x80', '').replace('_ex=128x128', '')

                    # ä»·æ ¼
                    price = extract_price(full_text)
                    if price == "0":
                        pt = item.select_one('[class*="price"]')
                        if pt:
                            price = extract_price(pt.get_text())

                    # è¯„è®º
                    rev_cnt, rev_score = 0, "0.0"
                    score_match = re.search(r'[â˜…â˜†è©•ä¾¡]\s*(\d\.\d{1,2})', full_text)
                    if not score_match:
                        score_match = re.search(r'(\d\.\d{1,2})\s*[\(ï¼ˆ]\d+[ä»¶\)ï¼‰]', full_text)
                    if score_match:
                        score_val = float(score_match.group(1))
                        if 1.0 <= score_val <= 5.0:
                            rev_score = score_match.group(1)

                    cnt_match = re.search(r'[\(ï¼ˆ](\d{1,6})[ä»¶]?[\)ï¼‰]', full_text)
                    if cnt_match:
                        rev_cnt = int(cnt_match.group(1))

                    # æ’å
                    is_ad = "[PR]" in title or item.select_one('.marker-pr') or "r.rakuten.co.jp" in link
                    if is_ad:
                        ad_rank += 1
                    else:
                        nat_rank += 1

                    sat = f"å•†å“æ•°{total_txt}ä»¶ä¸­, {('PR' if is_ad else 'è‡ªç„¶')}{ad_rank if is_ad else nat_rank}ä½"

                    raw_data.append({
                        "å“ç‰Œ": shop,
                        "æ ‡é¢˜": title,
                        "url": link,
                        "ä¸»å›¾": img,
                        "é¥±å’Œåº¦": sat,
                        "è¯„è®ºæ•°": rev_cnt,
                        "è¯„åˆ†": rev_score,
                        "ä»·æ ¼": int(price),
                        "ä¸Šçº¿æ—¶é•¿": "...",
                        "é¢„ä¼°æœˆé”€": "",
                        "å•†å“è¯¦ç»†å‚æ•°": "...",
                        "æ ¸å¿ƒå–ç‚¹åˆ†æ": "...",
                        "è¯„è®ºå‡ºç°ä¼˜ç‚¹": "...",
                        "å®¢è¯‰ç‚¹": "...",
                        "æœ‰æ— è§†é¢‘": "...",
                        "å¤§ç±»æ’å": "",
                        "å°ç±»æ’å": "",
                        "å¤‡æ³¨": "",
                        "ç‰¹å¾1": "", "ç‰¹å¾2": "", "ç‰¹å¾3": "", "ç‰¹å¾4": "", "ç‰¹å¾5": "",
                        "ç‰¹å¾6": "", "ç‰¹å¾7": "", "ç‰¹å¾8": "", "ç‰¹å¾9": ""
                    })
                except Exception as e:
                    log_message(task_id, f"   âš ï¸ å•†å“ {idx+1} è§£æå¤±è´¥: {e}")
                    continue

            task['progress'] = int((page / pages) * 20)
            log_message(task_id, f"   âœ… æœ¬é¡µè·å– {len(items)} æ¡æ•°æ®")
            time.sleep(1.5)

        if not raw_data:
            task['status'] = 'error'
            task['message'] = 'æœªæ‰¾åˆ°ä»»ä½•å•†å“'
            log_message(task_id, "âŒ æœªæ‰¾åˆ°ä»»ä½•å•†å“")
            return

        # ==================== Step 2: æ·±åº¦æŠ“å– ====================
        df = pd.DataFrame(raw_data)
        limit = min(len(df), deep_limit) if deep_limit else len(df)

        log_message(task_id, f"ğŸ•µï¸ å¼€å§‹æ·±åº¦æŠ“å– (å…± {limit} æ¡)...")

        for i in range(limit):
            row = df.iloc[i]
            is_pr = "PRå¹¿å‘Š" if "grp" in str(row['url']) else "æ™®é€š"
            task['message'] = f'æ·±åº¦åˆ†æ {i+1}/{limit}: {row["æ ‡é¢˜"][:25]}...'
            task['progress'] = 20 + int((i / limit) * 60)
            log_message(task_id, f"   [{i+1}/{limit}] åˆ†æä¸­... [{is_pr}]")

            # è°ƒç”¨åŸå§‹çˆ¬è™«çš„å®Œæ•´æ·±åº¦æŠ“å–å‡½æ•°
            details = get_product_details(row['url'], row['è¯„è®ºæ•°'])

            # æ£€æŸ¥è§£æç»“æœ
            if details['ä¸Šçº¿æ—¶é•¿'] == 'URLè§£æå¤±è´¥':
                log_message(task_id, f"      âš ï¸ URLè§£æå¤±è´¥")

            df.at[i, 'ä¸Šçº¿æ—¶é•¿'] = details['ä¸Šçº¿æ—¶é•¿']
            df.at[i, 'å•†å“è¯¦ç»†å‚æ•°'] = details['å•†å“è¯¦ç»†å‚æ•°']
            df.at[i, 'æ ¸å¿ƒå–ç‚¹åˆ†æ'] = details['æ ¸å¿ƒå–ç‚¹åˆ†æ']
            df.at[i, 'è¯„è®ºå‡ºç°ä¼˜ç‚¹'] = details['è¯„è®ºå‡ºç°ä¼˜ç‚¹']
            df.at[i, 'å®¢è¯‰ç‚¹'] = details['å®¢è¯‰ç‚¹']
            df.at[i, 'æœ‰æ— è§†é¢‘'] = details['æœ‰æ— è§†é¢‘']
            df.at[i, 'å¤§ç±»æ’å'] = details['å¤§ç±»æ’å']
            df.at[i, 'å°ç±»æ’å'] = details['å°ç±»æ’å']
            df.at[i, 'å¤‡æ³¨'] = details['å¤‡æ³¨']

            # äº§å“ç‰¹å¾
            if details.get('äº§å“ç‰¹å¾'):
                for j, feat in enumerate(details['äº§å“ç‰¹å¾'][:9], 1):
                    df.at[i, f'ç‰¹å¾{j}'] = feat
                log_message(task_id, f"      [AI] æå–åˆ° {len(details['äº§å“ç‰¹å¾'])} ä¸ªç‰¹å¾")

            # æ›´æ–°ä¸»å›¾
            if details.get('ä¸»å›¾') and (df.at[i, 'ä¸»å›¾'] == "N/A" or not df.at[i, 'ä¸»å›¾']):
                df.at[i, 'ä¸»å›¾'] = details['ä¸»å›¾']

            # è¡¥å……è¯„è®ºæ•°å’Œè¯„åˆ†
            if details.get('è¯„è®ºæ•°_è¡¥å……') is not None:
                try:
                    if int(df.at[i, 'è¯„è®ºæ•°']) == 0:
                        df.at[i, 'è¯„è®ºæ•°'] = details['è¯„è®ºæ•°_è¡¥å……']
                        log_message(task_id, f"      â†’ è¡¥å……è¯„è®ºæ•°: {details['è¯„è®ºæ•°_è¡¥å……']}")
                except:
                    df.at[i, 'è¯„è®ºæ•°'] = details['è¯„è®ºæ•°_è¡¥å……']

            if details.get('è¯„åˆ†_è¡¥å……') is not None:
                try:
                    if float(df.at[i, 'è¯„åˆ†']) == 0.0:
                        df.at[i, 'è¯„åˆ†'] = details['è¯„åˆ†_è¡¥å……']
                        log_message(task_id, f"      â†’ è¡¥å……è¯„åˆ†: {details['è¯„åˆ†_è¡¥å……']}")
                except:
                    df.at[i, 'è¯„åˆ†'] = details['è¯„åˆ†_è¡¥å……']

            time.sleep(random.uniform(1.2, 2.5))

        log_message(task_id, "âœ… æ·±åº¦æŠ“å–å®Œæˆ")

        # ==================== Step 3: æ•´ç†æ•°æ® ====================
        task['message'] = 'æ­£åœ¨æ•´ç†æ•°æ®æ ¼å¼...'
        task['progress'] = 85
        log_message(task_id, "ğŸ“Š æ•´ç†æ•°æ®æ ¼å¼...")

        # æ¸…ç†URL
        df['url'] = df['url'].apply(lambda x: x.split('|||')[0] if '|||' in str(x) else x)

        # å¤åˆ¶ç”¨äºç¿»è¯‘
        df_jp = df.copy()

        # æ·»åŠ å…³é”®è¯åˆ—
        df_jp['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'] = keyword
        df_jp['ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼'] = df_jp['æ ‡é¢˜']
        df_jp['å“ç‰Œï¼ˆåº—é“ºåï¼‰'] = df_jp['å“ç‰Œ']
        df_jp['å¸‚åœºé¥±å’Œåº¦'] = df_jp['é¥±å’Œåº¦']
        df_jp['reviewæ•°é‡'] = df_jp['è¯„è®ºæ•°']
        df_jp['reviewè¯„åˆ†'] = df_jp['è¯„åˆ†']
        df_jp['ä»·æ ¼ï¼ˆJPY)'] = df_jp['ä»·æ ¼']
        df_jp['ä¸Šçº¿æ—¶é•¿ï¼ˆæœˆï¼‰'] = df_jp['ä¸Šçº¿æ—¶é•¿']
        df_jp['æœˆé”€å”®é¢'] = ""

        # ç­–ç•¥åˆ†æåˆ—
        df_jp['é¢„ä¼°å”®ä»·'] = ""
        df_jp['ä¾›åº”å•†æ˜¯å¦å¯ä»¥å¼€å‘ç¥¨ï¼Œé‡‡è´­å¤šå°‘ä¸ªå¯ä»¥å¼€'] = ""
        df_jp['å¹³å‡æ¯›åˆ©ç‡'] = ""
        df_jp['ä¿ƒé”€é¢‘ç‡'] = ""
        df_jp['å¯ä¼˜åŒ–æ–¹å‘'] = ""
        df_jp['ä¼˜å…ˆçº§'] = ""

        # æœ€ç»ˆåˆ—é¡ºåº
        final_cols = [
            "å“ç‰Œï¼ˆåº—é“ºåï¼‰", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼", "url", "ä¸»å›¾",
            "å¸‚åœºé¥±å’Œåº¦", "å°ç±»æ’å", "å¤§ç±»æ’å", "reviewæ•°é‡", "reviewè¯„åˆ†",
            "ä»·æ ¼ï¼ˆJPY)", "ä¸Šçº¿æ—¶é•¿ï¼ˆæœˆï¼‰", "é¢„ä¼°æœˆé”€", "æœˆé”€å”®é¢",
            "ç‰¹å¾1", "ç‰¹å¾2", "ç‰¹å¾3", "ç‰¹å¾4", "ç‰¹å¾5", "ç‰¹å¾6", "ç‰¹å¾7", "ç‰¹å¾8", "ç‰¹å¾9",
            "æ ¸å¿ƒå–ç‚¹åˆ†æ", "è¯„è®ºå‡ºç°ä¼˜ç‚¹", "å®¢è¯‰ç‚¹", "æœ‰æ— è§†é¢‘", "å¤‡æ³¨",
            "é¢„ä¼°å”®ä»·", "ä¾›åº”å•†æ˜¯å¦å¯ä»¥å¼€å‘ç¥¨ï¼Œé‡‡è´­å¤šå°‘ä¸ªå¯ä»¥å¼€", "å¹³å‡æ¯›åˆ©ç‡", "ä¿ƒé”€é¢‘ç‡", "å¯ä¼˜åŒ–æ–¹å‘", "ä¼˜å…ˆçº§"
        ]

        # ç¡®ä¿åˆ—å­˜åœ¨
        for col in final_cols:
            if col not in df_jp.columns:
                df_jp[col] = ""

        df_jp = df_jp.reindex(columns=final_cols)

        # ä¸­æ–‡ç¿»è¯‘ç‰ˆ
        df_cn = df_jp.copy()
        if enable_translate and ENABLE_TRANSLATION:
            task['message'] = 'æ­£åœ¨ç¿»è¯‘ä¸­æ–‡ç‰ˆ...'
            task['progress'] = 90
            log_message(task_id, "ğŸ‡¨ğŸ‡³ æ­£åœ¨ç¿»è¯‘ä¸­æ–‡ç‰ˆ...")

            cols_to_translate = ["å“ç‰Œï¼ˆåº—é“ºåï¼‰", "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼", "å¸‚åœºé¥±å’Œåº¦",
                                 "æ ¸å¿ƒå–ç‚¹åˆ†æ", "è¯„è®ºå‡ºç°ä¼˜ç‚¹", "å®¢è¯‰ç‚¹",
                                 "ç‰¹å¾1", "ç‰¹å¾2", "ç‰¹å¾3", "ç‰¹å¾4", "ç‰¹å¾5",
                                 "ç‰¹å¾6", "ç‰¹å¾7", "ç‰¹å¾8", "ç‰¹å¾9"]

            for idx, col in enumerate(cols_to_translate):
                if col in df_cn.columns:
                    log_message(task_id, f"   ç¿»è¯‘åˆ—: {col}")
                    try:
                        df_cn[col] = df_cn[col].apply(safe_translate)
                    except Exception as e:
                        log_message(task_id, f"   âš ï¸ ç¿»è¯‘ {col} å¤±è´¥: {e}")

            # ç¿»è¯‘æ’å
            def translate_rank(rank_text):
                if not rank_text or not isinstance(rank_text, str):
                    return rank_text
                match = re.match(r'(.+?)\s*ç¬¬(\d+)', rank_text)
                if match:
                    cat_name_jp = match.group(1)
                    rank_num = match.group(2)
                    cat_name_cn = safe_translate(cat_name_jp)
                    return f"{cat_name_cn} ç¬¬{rank_num}"
                return rank_text

            df_cn['å¤§ç±»æ’å'] = df_cn['å¤§ç±»æ’å'].apply(translate_rank)
            df_cn['å°ç±»æ’å'] = df_cn['å°ç±»æ’å'].apply(translate_rank)
            log_message(task_id, "   âœ… ç¿»è¯‘å®Œæˆ")

        # ä¿å­˜ç»“æœ
        task['progress'] = 95
        task['message'] = 'ä¿å­˜ç»“æœ...'

        # è½¬æ¢ä¸ºç»“æœåˆ—è¡¨
        results = df.to_dict('records')
        task['results'] = results
        task['dataframe_jp'] = df_jp
        task['dataframe_cn'] = df_cn
        task['keyword'] = keyword
        task['status'] = 'completed'
        task['progress'] = 100
        task['message'] = f'å®Œæˆï¼å…±æŠ“å– {len(results)} æ¡å•†å“æ•°æ®'
        log_message(task_id, f"ğŸ‰ å®Œæˆï¼å…±æŠ“å– {len(results)} æ¡å•†å“æ•°æ®")

    except Exception as e:
        task['status'] = 'error'
        task['message'] = f'çˆ¬è™«é”™è¯¯: {str(e)}'
        log_message(task_id, f"âŒ çˆ¬è™«é”™è¯¯: {str(e)}")
        import traceback
        log_message(task_id, traceback.format_exc())


# ==================== API è·¯ç”± ====================

@app.route('/')
def index():
    """é¦–é¡µ"""
    return render_template('index_full.html')


@app.route('/api/scrape', methods=['POST'])
def start_scrape():
    """å¯åŠ¨çˆ¬è™«ä»»åŠ¡"""
    data = request.json
    keyword = data.get('keyword', '').strip()
    pages = int(data.get('pages', 1))
    deep_limit = int(data.get('deep_limit', 5))
    enable_ai = data.get('enable_ai', True)
    enable_translate = data.get('enable_translate', True)

    if not keyword:
        return jsonify({'error': 'è¯·è¾“å…¥å…³é”®è¯'}), 400

    task_id = str(uuid.uuid4())
    tasks[task_id] = {
        'id': task_id,
        'keyword': keyword,
        'status': 'pending',
        'progress': 0,
        'message': 'ä»»åŠ¡å·²åˆ›å»º',
        'results': [],
        'dataframe_jp': None,
        'dataframe_cn': None,
        'created_at': datetime.now().isoformat()
    }
    task_logs[task_id] = []

    # å¯åŠ¨åå°çº¿ç¨‹
    thread = threading.Thread(
        target=run_scraper_full,
        args=(task_id, keyword, pages, deep_limit, enable_ai, enable_translate)
    )
    thread.daemon = True
    thread.start()

    return jsonify({'task_id': task_id})


@app.route('/api/task/<task_id>')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    task = tasks.get(task_id)
    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

    return jsonify({
        'id': task['id'],
        'keyword': task.get('keyword', ''),
        'status': task['status'],
        'progress': task['progress'],
        'message': task['message'],
        'results': task.get('results', []),
        'result_count': len(task.get('results', []))
    })


@app.route('/api/logs/<task_id>')
def get_task_logs(task_id):
    """è·å–ä»»åŠ¡æ—¥å¿—"""
    logs = task_logs.get(task_id, [])
    return jsonify({'logs': logs})


@app.route('/api/download/<task_id>/<lang>')
def download_excel(task_id, lang='jp'):
    """ä¸‹è½½Excelæ–‡ä»¶"""
    task = tasks.get(task_id)
    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

    if task['status'] != 'completed':
        return jsonify({'error': 'ä»»åŠ¡æœªå®Œæˆ'}), 400

    df = task.get('dataframe_cn' if lang == 'cn' else 'dataframe_jp')
    if df is None:
        return jsonify({'error': 'æ•°æ®ä¸å­˜åœ¨'}), 400

    # åˆ›å»ºExcel
    output = io.BytesIO()

    try:
        wb = Workbook()
        ws = wb.active
        ws.title = "çˆ¬è™«ç»“æœ"

        # æ ·å¼
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF")
        group_fill = PatternFill(start_color="D9E1F2", end_color="D9E1F2", fill_type="solid")
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )

        # åˆ†ç»„ä¿¡æ¯
        groups = [
            ("åŸºæœ¬ä¿¡æ¯", 5),
            ("å”®å–ä¿¡æ¯", 9),
            ("äº§å“ç‰¹å¾ä¸ç”¨æˆ·ä½“éªŒ", 14),
            ("ç­–ç•¥åˆ†æä¸å¤ç›˜", 6)
        ]

        # å†™å…¥åˆ†ç»„æ ‡é¢˜è¡Œ
        col_idx = 1
        for group_name, group_cols in groups:
            cell = ws.cell(row=1, column=col_idx, value=group_name)
            cell.font = Font(bold=True)
            cell.fill = group_fill
            cell.alignment = Alignment(horizontal='center', vertical='center')
            cell.border = thin_border
            if group_cols > 1:
                ws.merge_cells(start_row=1, start_column=col_idx, end_row=1, end_column=col_idx + group_cols - 1)
            col_idx += group_cols

        # å†™å…¥åˆ—å¤´
        for c_idx, col_name in enumerate(df.columns, 1):
            cell = ws.cell(row=2, column=c_idx, value=col_name)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(wrap_text=True, horizontal='center', vertical='center')
            cell.border = thin_border

        # å†™å…¥æ•°æ®
        for r_idx, row in enumerate(df.values, 3):
            for c_idx, value in enumerate(row, 1):
                cell = ws.cell(row=r_idx, column=c_idx, value=value)
                cell.alignment = Alignment(wrap_text=True, vertical='top')
                cell.border = thin_border

        # åˆ—å®½
        col_widths = {
            "å“ç‰Œï¼ˆåº—é“ºåï¼‰": 18, "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": 15, "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼": 35, "url": 25, "ä¸»å›¾": 20,
            "å¸‚åœºé¥±å’Œåº¦": 25, "å°ç±»æ’å": 15, "å¤§ç±»æ’å": 15, "reviewæ•°é‡": 10, "reviewè¯„åˆ†": 10,
            "ä»·æ ¼ï¼ˆJPY)": 12, "ä¸Šçº¿æ—¶é•¿ï¼ˆæœˆï¼‰": 14, "é¢„ä¼°æœˆé”€": 12, "æœˆé”€å”®é¢": 12,
            "ç‰¹å¾1": 18, "ç‰¹å¾2": 18, "ç‰¹å¾3": 18, "ç‰¹å¾4": 18, "ç‰¹å¾5": 18,
            "ç‰¹å¾6": 18, "ç‰¹å¾7": 18, "ç‰¹å¾8": 18, "ç‰¹å¾9": 18,
            "æ ¸å¿ƒå–ç‚¹åˆ†æ": 35, "è¯„è®ºå‡ºç°ä¼˜ç‚¹": 35, "å®¢è¯‰ç‚¹": 35, "æœ‰æ— è§†é¢‘": 10, "å¤‡æ³¨": 20,
            "é¢„ä¼°å”®ä»·": 12, "ä¾›åº”å•†æ˜¯å¦å¯ä»¥å¼€å‘ç¥¨ï¼Œé‡‡è´­å¤šå°‘ä¸ªå¯ä»¥å¼€": 25,
            "å¹³å‡æ¯›åˆ©ç‡": 12, "ä¿ƒé”€é¢‘ç‡": 12, "å¯ä¼˜åŒ–æ–¹å‘": 20, "ä¼˜å…ˆçº§": 10
        }

        for c_idx, col_name in enumerate(df.columns, 1):
            col_letter = get_column_letter(c_idx)
            ws.column_dimensions[col_letter].width = col_widths.get(col_name, 15)

        # è¡Œé«˜
        ws.row_dimensions[1].height = 25
        ws.row_dimensions[2].height = 30
        for row_idx in range(3, ws.max_row + 1):
            ws.row_dimensions[row_idx].height = 60

        # å†»ç»“
        ws.freeze_panes = 'A3'

        wb.save(output)
        output.seek(0)

    except Exception as e:
        return jsonify({'error': f'Excelç”Ÿæˆå¤±è´¥: {str(e)}'}), 500

    lang_suffix = "CN" if lang == 'cn' else "JP"
    filename = f"rakuten_{task['keyword']}_{lang_suffix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

    return send_file(
        output,
        mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        as_attachment=True,
        download_name=filename
    )


@app.route('/api/config')
def get_config():
    """è·å–å½“å‰é…ç½®"""
    return jsonify({
        'proxy_port': PROXY_PORT,
        'ai_enabled': USE_AI_FEATURES,
        'translation_enabled': ENABLE_TRANSLATION
    })


if __name__ == '__main__':
    os.makedirs('templates', exist_ok=True)
    print("=" * 60)
    print("ğŸš€ ä¹å¤©å•†å“çˆ¬è™« Web åº”ç”¨ - å®Œæ•´ç‰ˆ")
    print(f"ğŸ“¡ ä»£ç†ç«¯å£: {PROXY_PORT}")
    print(f"ğŸ¤– AIç‰¹å¾æå–: {'å¯ç”¨' if USE_AI_FEATURES else 'ç¦ç”¨'}")
    print(f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘åŠŸèƒ½: {'å¯ç”¨' if ENABLE_TRANSLATION else 'ç¦ç”¨'}")
    print("=" * 60)
    print("è®¿é—® http://localhost:5000 å¼€å§‹ä½¿ç”¨")
    print("=" * 60)
    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)
